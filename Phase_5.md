# ä»é›¶å®ç° FlashAttentionï¼ˆPhase 5ï¼‰ï¼šæ€§èƒ½è°ƒä¼˜ â€”â€” ä»èƒ½è·‘åˆ°é£èµ·æ¥

ç›®æ ‡ï¼š

ç†è§£å¹¶ä½¿ç”¨ triton çš„æ ¸å¿ƒä¼˜åŒ–æŠ€å·§ï¼Œè®©æˆ‘ä»¬çš„ç®—å­ä»â€œèƒ½è·‘â€åˆ°â€œé£èµ·æ¥â€ï¼Œå°½å¯èƒ½é€¼è¿‘ Pytorch å®˜æ–¹å®ç°

å®Œæ•´ä»£ç è¯·å‚è€ƒï¼š[FlashAttention-from-Scratch-with-Triton/code at main Â· Pearbiossom-M/FlashAttention-from-Scratch-with-Triton](https://github.com/Pearbiossom-M/FlashAttention-from-Scratch-with-Triton/tree/main/code)

------

## 5.1 å»ºç«‹æ€§èƒ½åŸºçº¿

ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦æµ‹é‡å½“å‰ç®—å­çš„æ€§èƒ½ä½œä¸ºåŸºçº¿ï¼Œä»¥éªŒè¯åç»­ä¼˜åŒ–æ˜¯å¦æœ‰æ•ˆã€‚è¿™é‡Œè®¾ç½®ï¼š

* `B = 4`ï¼Œ`H = 8`
* å¤´ç»´åº¦ `D = 64`
* `is_causal=True`
* è®¡ç®—å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­çš„ **TFLOPS** ä½œä¸ºæ€§èƒ½æŒ‡æ ‡

> ä¸ºäº†é¿å…è¢«è¿‡å¤šé…ç½®æ·¹æ²¡ï¼Œè¿™é‡Œé€‰æ‹©ä¸Šè¿°é…ç½®ä½œä¸ºä»£è¡¨æ€§ workloadï¼Œä»¥å±•ç¤ºæ¯ä¸€æ­¥çš„ä¼˜åŒ–æ”¶ç›Šã€‚å½“æ‰€æœ‰ä¼˜åŒ–å®Œæˆåï¼Œæˆ‘ä»¬å†å¯¹ `Dâˆˆ{64,128} Ã— is_causalâˆˆ{True,False} Ã— modeâˆˆ{fwd,bwd,fwd_bwd}` çš„å®Œæ•´é…ç½®è¿›è¡Œç»Ÿä¸€è¯„æµ‹ï¼Œå¹¶åœ¨æœ¬ç« æœ«ç»™å‡ºæ€»è§ˆå›¾

> è¿™é‡Œä½¿ç”¨ RTX 5060ti 16G è¿›è¡Œå®éªŒï¼Œç”±äºGPUå†…å­˜é™åˆ¶ï¼Œæ‰€æœ‰å‰å‘+åå‘ï¼ˆfwd+bwdï¼‰åŸºå‡†æµ‹è¯•å‡åœ¨å¤´ç»´åº¦ D=64 çš„æ¡ä»¶ä¸‹è¿›è¡Œï¼Œä½†ä½¿ç”¨çš„ä¼˜åŒ–ç­–ç•¥åŒæ ·é€‚ç”¨äº D=128

TFLOPS è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

```python
# è®¡ç®— TFLOPS
# å‚è€ƒï¼šhttps://github.com/Dao-AILab/flash-attention/blob/main/benchmarks/benchmark_flash_attention.py
# Forward: 4 * B * H * S_q * S_k * D (2 matmuls: Q@K^T, P@V, each costs 2*S_q*S_k*D FLOPs)
# Backward: å¤§çº¦æ˜¯ Forward çš„ 2.5 å€
flops = 4 * B * H * S_q * S_k * D // (2 if is_causal else 1)
if mode == 'fwd':
    tflops = flops / (avg_time_ms * 1e-3) / 1e12
elif mode == 'bwd':
    tflops = 2.5 * flops / (avg_time_ms * 1e-3) / 1e12  # 2.5x forward
else: # fwd_bwd
    tflops = 3.5 * flops / (avg_time_ms * 1e-3) / 1e12  # 1x forward + 2.5x forward
```

å…¶ä¸­ `avg_time_ms` ç”±å¦‚ä¸‹ `timing` å‡½æ•°è·å–ï¼Œæ³¨æ„ï¼š

* GPU kernel çš„æ‰§è¡Œæ˜¯å¼‚æ­¥çš„ï¼Œä½¿ç”¨ CPU ä¾§çš„ `time.time()` å¾€å¾€åªèƒ½æµ‹åˆ° kernel çš„ launch å¼€é”€ï¼Œè€Œæ— æ³•åæ˜ çœŸå®çš„æ‰§è¡Œæ—¶é—´ã€‚å› æ­¤æœ¬æ–‡é‡‡ç”¨ `torch.cuda.Event` åœ¨ GPU æ‰§è¡Œæµä¸­æ’å…¥æ—¶é—´æˆ³ï¼Œç”± GPU è‡ªèº«è®°å½•èµ·æ­¢æ—¶é—´ï¼Œå¹¶åœ¨åŒæ­¥åè¯»å–ä¸¤è€…çš„é—´éš”ï¼Œä»è€Œè·å¾—å‡†ç¡®çš„ GPU ç«¯æ‰§è¡Œæ—¶å»¶ã€‚
* å¦ä¸€æ–¹é¢ï¼Œé¦–æ¬¡è°ƒç”¨ kernel æ—¶é€šå¸¸ä¼šè§¦å‘ç¼–è¯‘ã€è‡ªåŠ¨è°ƒä¼˜ä»¥åŠç¼“å­˜å†·å¯åŠ¨ç­‰ä¸€æ¬¡æ€§å¼€é”€ï¼Œè¿™äº›å¹¶ä¸å±äº steady-state æ€§èƒ½ã€‚ä¸ºé¿å…è¿™ç±»å™ªå£°å¯¹ç»“æœçš„å½±å“ï¼Œè®¡æ—¶å‰å…ˆæ‰§è¡Œ warmupï¼Œä½¿ GPU è¿›å…¥ç¨³å®šè¿è¡ŒçŠ¶æ€ï¼Œæœ€ç»ˆæŠ¥å‘Šçš„æ—¶é—´ä¸ºå¤šæ¬¡é‡å¤æ‰§è¡Œåçš„å¹³å‡å€¼ã€‚

```python
def timing(run_fn, warmup=10, repeat=30):
    # 1. Warmupï¼šè¿›å…¥ steady state
    for _ in range(warmup):
        run_fn()

    starter = torch.cuda.Event(enable_timing=True)
    ender = torch.cuda.Event(enable_timing=True)
    
	# 2. ç¡®ä¿ä¹‹å‰çš„å¼‚æ­¥å·¥ä½œå®Œæˆ
    torch.cuda.synchronize()
    
    # 3. åœ¨ GPU stream ä¸­æ’å…¥èµ·æ­¢æ—¶é—´æˆ³
    starter.record()
    for _ in range(repeat):
        run_fn()
    ender.record()
    
    # 4. ç­‰ GPU æ‰§è¡Œå®Œæˆï¼Œè¯»å–çœŸå®è€—æ—¶
    torch.cuda.synchronize()
    elapsed = starter.elapsed_time(ender)
    
    # 5. è¿”å›å•æ¬¡å¹³å‡æ—¶é—´ (ms)
    avg_time_ms = elapsed / repeat
    return avg_time_ms
```

è¿™é‡Œæ¯”è¾ƒäº†ä¸‰ç§ attention å®ç°ï¼š

* **Naive attention**ï¼šçº¯ Pythonï¼Œæ ‡å‡†çš„ `QKáµ€ â†’ softmax â†’ PV` å®ç°ï¼Œä¸è¿›è¡Œä»»ä½•æ˜¾å­˜ä¼˜åŒ–
* **Triton FlashAttentionï¼ˆæœ¬æ–‡å®ç°ï¼‰**ï¼šåŸºäº online softmax çš„æµå¼è®¡ç®—
* **PyTorch SDPAï¼ˆå®˜æ–¹å®ç°ï¼‰**ï¼šé«˜åº¦ä¼˜åŒ–çš„å·¥ä¸šçº§å®ç°ï¼Œä½œä¸ºæ€§èƒ½å‚è€ƒä¸Šé™

ç»“æœå¦‚ä¸‹ï¼š

<img src="./images/D_64_causal_fwd_bwd.png" style="zoom: 33%;" />

* **Naive**ï¼š
  
  * ååç‡éšåºåˆ—é•¿åº¦ä¸‹é™ï¼Œè¯´æ˜è¯¥å®ç°å·²æ˜æ˜¾å—åˆ°æ˜¾å­˜è®¿å­˜å’Œä¸­é—´çŸ©é˜µç‰©åŒ–ï¼ˆmaterializationï¼‰çš„é™åˆ¶
  * å¤§åºåˆ—ç›´æ¥ä¸å¯ç”¨ï¼ˆOOMï¼‰ï¼Œè¿™æ­£æ˜¯ FlashAttention è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜
  * **ç»“è®º**ï¼šnaive attention æ—¢æ…¢ã€åˆä¸å…·å¤‡å¯æ‰©å±•æ€§ï¼Œåªèƒ½ä½œä¸ºåŠŸèƒ½æ­£ç¡®æ€§çš„å‚è€ƒã€‚
* **Triton FlashAttention**ï¼š

  * **ä½åºåˆ—é•¿åº¦**ï¼šè®¡ç®—èµ„æºåˆ©ç”¨ç‡ä¸è¶³

    åœ¨åºåˆ—é•¿åº¦è¾ƒå°æ—¶ï¼ŒK/V Blocks æ•°é‡å°‘ï¼Œæ¯ä¸ª Q Block ç»å†çš„**å¾ªç¯æ¬¡æ•°æœ‰é™**ï¼š

    - ç®—æœ¯å¼ºåº¦è¾ƒä½ï¼šQ Block ä» HBM æ¬å…¥åï¼Œä»…ä¸å°‘é‡ K/V Blocks è¿›è¡Œè®¡ç®—
    - **Block ç”Ÿå‘½å‘¨æœŸçŸ­**ï¼šæ¯ä¸ª Block å¾ˆå¿«æ‰§è¡Œå®Œæ¯•
    - **å›ºå®šå¼€é”€å æ¯”é«˜**ï¼šæ¯ä¸ª Block çš„è°ƒåº¦ã€å¯åŠ¨ã€æµæ°´çº¿å¡«å……ç­‰å›ºå®šæˆæœ¬åœ¨æ€»æ—¶é—´ä¸­å æ¯”è¾ƒå¤§

    **SM** éš¾ä»¥é•¿æ—¶é—´ç»´æŒé«˜æ•ˆæ‰§è¡ŒçŠ¶æ€ï¼Œè®¡ç®—èµ„æºåˆ©ç”¨ç‡ä½ï¼Œæ­¤é˜¶æ®µå¯ä»¥ç†è§£ä¸ºâ€œ**ä½ç®—æœ¯å¼ºåº¦ + ä½è®¡ç®—èµ„æºåˆ©ç”¨ç‡**â€çš„åŒºåŸŸã€‚

  * **ä¸­åºåˆ—é•¿åº¦**ï¼šæ€§èƒ½éšåºåˆ—é•¿åº¦å¢åŠ è€Œå¢åŠ 

    éšç€åºåˆ—é•¿åº¦å¢å¤§ï¼ŒåŒä¸€ä¸ª Q Block éœ€è¦ä¸æ›´å¤šçš„ K/V Blocks è¿›è¡Œè®¡ç®—ï¼Œå†…éƒ¨å¾ªç¯æ¬¡æ•°å¢åŠ ï¼š

    - ç®—æœ¯å¼ºåº¦æå‡ï¼šQ åœ¨ç‰‡ä¸Šåœç•™æ—¶é—´å˜é•¿ï¼Œæ¯æ¬¡åŠ è½½çš„ Q åœ¨ç‰‡ä¸Šè¢«ä½¿ç”¨æ›´å¤šæ¬¡
    - å•ä¸ª **Block ç”Ÿå‘½å‘¨æœŸå˜é•¿**ï¼š å›ºå®šå¼€é”€è¢«æ‘Šè–„ï¼Œè®¡ç®—å•å…ƒçš„åˆ©ç”¨ç‡æ˜¾è‘—æå‡

    > è¿™æœ‰ä¸€ç‚¹ç±»ä¼¼äº **Persistent Block** çš„åŸç†

  * **é•¿åºåˆ—é•¿åº¦**ï¼šè¿›å…¥å®ç°å¹³å°åŒº

    å½“åºåˆ—é•¿åº¦è¶³å¤Ÿå¤§æ—¶ï¼š

    - block ç”Ÿå‘½å‘¨æœŸå·²ç»å¾ˆé•¿ï¼Œ**è°ƒåº¦å¼€é”€å‡ ä¹å¯å¿½ç•¥**
    - è®¡ç®—èµ„æºåˆ©ç”¨ç‡æ¥è¿‘å½“å‰å®ç°çš„ä¸Šé™ï¼šå†å¢åŠ åºåˆ—é•¿åº¦ï¼Œä¹Ÿåªæ˜¯å¢åŠ è®¡ç®—æ—¶é—´ï¼Œå°½ç®¡ç®—æœ¯å¼ºåº¦èƒ½ç»§ç»­æé«˜ï¼Œä½†ä¸ä¼šå†æå‡è®¡ç®—èµ„æºåˆ©ç”¨ç‡
    - è¿™ä¸ªå¹³å°åŒº**å¹¶ä¸ä¸€å®šæ˜¯â€œç¡¬ä»¶ç†è®ºå³°å€¼â€**ï¼Œè€Œåªæ˜¯å½“å‰ Triton å®ç°çš„æ€§èƒ½ä¸Šé™

  * æ²¡æœ‰ OOMï¼Œå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§

  * **ç»“è®º**ï¼šTriton FlashAttention çš„æ€§èƒ½æ¼”åŒ–æœ¬è´¨ä¸Šæ˜¯ï¼šä»â€œå°è§„æ¨¡æ—¶è®¡ç®—èµ„æºæœªå……åˆ†åˆ©ç”¨â€è¿‡æ¸¡åˆ°â€œå¤§è§„æ¨¡ä¸‹è®¡ç®—å•å…ƒæŒç»­é«˜è´Ÿè½½è¿è¡Œâ€ã€‚å…¶ä¼˜åŠ¿åœ¨äºæ˜¾è‘—æé«˜ç®—æœ¯å¼ºåº¦ï¼Œä»¥åŠä½¿ç®—å­åœ¨å¤§è§„æ¨¡ä¸‹èƒ½å¤Ÿå……åˆ†é©±åŠ¨è®¡ç®—èµ„æºï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

  > å¹³å°åŒºè¡¨æ˜ç®—å­å·²æ¥è¿‘å½“å‰å®ç°çš„ä¸Šç•Œï¼Œè€Œéç»å¯¹ç¡¬ä»¶å³°å€¼ï¼Œå› ä¸º PyTorch å®˜æ–¹å®ç°æ˜æ˜¾æ€§èƒ½æ›´å¥½ã€‚
  >
  > è¿›ä¸€æ­¥æå‡éœ€è¦ç®—æ³•å±‚ä¸è°ƒåº¦å±‚çš„ååŒä¼˜åŒ–ï¼Œè€Œä¸ä»…ä»…æ˜¯å¢å¤§é—®é¢˜è§„æ¨¡ã€‚

* **PyTorch SDPA**ï¼š
  
  - æ•´ä½“æ€§èƒ½æœ€é«˜ï¼ŒTriton åœ¨å¹³å°åŒºçº¦ä¸º PyTorch çš„ **55â€“60%**
  - è¯¥ç»“æœéªŒè¯äº†æµ‹è¯•æ–¹æ³•ä¸ FLOPs ä¼°ç®—çš„åˆç†æ€§
  - **ç»“è®º**ï¼šPyTorch SDPA æä¾›äº†ä¸€ä¸ªå¯ä¿¡çš„â€œå·¥ä¸šçº§å‚è€ƒä¸Šé™â€ï¼Œç”¨äºè¡¡é‡è‡ªå®šä¹‰å®ç°çš„ä¼˜åŒ–ç©ºé—´ã€‚



------

## 5.2 Auto-tune

åœ¨ä¹‹å‰çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨**åŠŸèƒ½æ­£ç¡®æ€§**ï¼Œä¸åœ¨ä¹ BLOCK_M ç­‰å‚æ•°æ˜¯å¦ä¼šå½±å“æ€§èƒ½å‘æŒ¥ï¼Œæ‰€ä»¥å¯ä»¥æŠŠå‚æ•°å†™æ­»ã€‚ä½†è¿›å…¥æ€§èƒ½è°ƒä¼˜é˜¶æ®µåï¼Œç”±äº triton çš„å¾ˆå¤šä¼˜åŒ–æ‰‹æ®µå¯¹ BLOCK_M / BLOCK_N / num_warps / num_stages æ¯”è¾ƒ**æ•æ„Ÿ**ï¼Œå¦‚æœè¿˜å›ºå®šè¿™äº›å‚æ•°ï¼Œå½“å‚æ•°ä¸åŒ¹é…æ—¶ï¼Œå¾ˆå¯èƒ½å¯¼è‡´åº”ç”¨æŸäº›ä¼˜åŒ–æ‰‹æ®µåï¼Œæ€§èƒ½ä¸å‡åé™ã€‚å› æ­¤ä¸ºäº†é¿å…å‚æ•°ä¸åŒ¹é…å¯¼è‡´çš„å¹²æ‰°ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œ auto-tuneã€‚

### 5.2.1 kernel æ€§èƒ½å—å“ªäº›å‚æ•°å½±å“ï¼Ÿ

* æ¯æ¬¡æ¬è¿çš„ tile å¤§å°ï¼ˆ**BLOCK_M / BLOCK_N**ï¼‰

  * **å†…å­˜å‹åŠ›**ï¼štile è¶Šå¤§ï¼Œå•æ¬¡æ¬è¿æ›´åˆ’ç®—ï¼Œä½† shared/register å‹åŠ›æ›´å¤§

  * **å ç”¨ç‡ï¼ˆoccupancyï¼‰**ï¼šshared/register å‹åŠ›å¤§ï¼Œå¯¼è‡´æ´»è·ƒ warps ä¸‹é™ï¼Œé™ä½å ç”¨ç‡

  * **æµæ°´çº¿æ°”æ³¡**ï¼šå ç”¨ç‡é™ä½ï¼Œæµæ°´çº¿æ›´éš¾è¢«å¡«æ»¡ï¼ˆå‡ºç°æ°”æ³¡ï¼‰ï¼Œå»¶è¿Ÿéš¾ä»¥éšè—ï¼Œtensor core æ‰“ä¸æ»¡

* çº¿ç¨‹æŸæ•°é‡ï¼ˆ**num_warps**ï¼‰
  * **å»¶è¿Ÿéšè—**ï¼šæ›´å¤šçš„ warps æ„å‘³ç€ç¡¬ä»¶æœ‰æ›´å¤šçš„è°ƒåº¦é€‰æ‹©ï¼Œæ›´å®¹æ˜“éšè—æµæ°´çº¿å»¶è¿Ÿ
  * **å¯„å­˜å™¨å‹åŠ›**ï¼šè¿™æ˜¯å…³é”®çš„è´Ÿåé¦ˆã€‚æ¯ä¸ª SMï¼ˆæµå¼å¤šå¤„ç†å™¨ï¼‰çš„å¯„å­˜å™¨æ€»é‡æ˜¯å›ºå®šçš„ã€‚`num_warps` è¶Šå¤šï¼Œæ¯ä¸ªçº¿ç¨‹èƒ½åˆ†é…åˆ°çš„å¯„å­˜å™¨å°±è¶Šå°‘
    * å¦‚æœå¯„å­˜å™¨ä¸è¶³ï¼Œç¼–è¯‘å™¨ä¼šå‘ç”Ÿ **Register Spilling**ï¼ˆå°†å¯„å­˜å™¨æº¢å‡ºåˆ°æ…¢é€Ÿæ˜¾å­˜ï¼‰ï¼Œå¯¼è‡´æ€§èƒ½é›ªå´©
    * åœ¨åå‘ä¼ æ’­ï¼ˆBackwardï¼‰ä¸­ï¼Œç”±äºéœ€è¦å­˜æ›´å¤šçš„ä¸­é—´å˜é‡ï¼ˆdQ, dK, dV ç­‰ï¼‰ï¼Œå¯„å­˜å™¨å‹åŠ›æ¯”å‰å‘å¤§å¾—å¤šï¼Œå› æ­¤é€šå¸¸éœ€è¦æ¯”å‰å‘æ›´å°çš„ `BLOCK` æˆ–æ›´ç²¾ç»†çš„ `num_warps`
* æµæ°´çº¿é˜¶æ®µæ•°ï¼ˆ**num_stages**ï¼‰
  * å¼•å…¥ TMAï¼ˆTensor Memory Acceleratorï¼‰åï¼Œ`num_stages` å®šä¹‰äº†åœ¨ Shared Memory ä¸­å¼€è¾Ÿçš„â€œç¼“å†²åŒºâ€æ•°é‡
  * `num_stages` è¶Šå¤§ï¼Œæµæ°´çº¿è¶Šä¸å®¹æ˜“å‡ºç°æ°”æ³¡ï¼Œè®¡ç®—å•å…ƒå°±è¶Šä¸å®¹æ˜“å› ä¸ºç­‰å¾…æ•°æ®è€Œâ€œé¥¿æ­»â€
  * **Shared Memory é™åˆ¶**ï¼šè¿™æ˜¯ç¡¬ä»¶é™åˆ¶ï¼Œå¦‚æœ `num_stages` å¤ªå¤§ï¼ŒShared Memory ææ˜“è€—å°½ï¼Œå¯¼è‡´ kernel æ— æ³•å¯åŠ¨æˆ– Occupancy æä½ï¼Œæ€§èƒ½é›ªå´©

### 5.2.2 Auto-tune çš„å®ç°

FlashAttention çš„åå‘ä¼ æ’­**è®¡ç®—æ›´é‡ã€å¯„å­˜å™¨æ›´ç´§**ï¼Œå› æ­¤å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„æœ€ä½³å‚æ•°é…ç½®ä¸€èˆ¬ä¸ç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹äºŒè€…**åˆ†åˆ«è¿›è¡Œ autotune**ã€‚

è°ƒä¼˜é…ç½®è®¾ç½®å¦‚ä¸‹ï¼š

```python
@triton.autotune(
    configs = [
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=8, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=3),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=3),
    ],
    key=['S_q', 'S_k', 'D', 'is_causal'],  # æ ¹æ®è¿™äº›å‚æ•°é€‰æ‹©æœ€ä¼˜é…ç½®
)

@triton.jit
def flash_attention_forward_kernel(...)
```

> æ³¨æ„ï¼š`@triton.autotune` éœ€è¦ä¸ kernel æ”¾åœ¨ä¸€èµ·ï¼Œä¸”ä¸€ç»„é…ç½®åªè´Ÿè´£ä¸€ä¸ªkernelï¼Œå³æˆ‘ä»¬è¿™é‡Œçš„ `flash_attention_forward_kernel`ã€`flash_attention_dQ_kernel` å’Œ `flash_attention_dKV_kernel` éƒ½éœ€è¦å•ç‹¬å†™ä¸€ä¸ª `@triton.autotune`ã€‚

åœ¨ä½¿ç”¨ `triton.autotune` æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠ `grid` å†™æˆä¸€ä¸ª `lambda META: ...` çš„å½¢å¼ï¼š

- `META` ç”± Triton åœ¨æ¯æ¬¡ autotune è¯•è·‘æŸä¸ª `triton.Config` æ—¶è‡ªåŠ¨ä¼ å…¥
- `META` ä¸­ä¼šåŒ…å«è¯¥ config çš„ tile å‚æ•°ï¼ˆå¦‚ `BLOCK_M`, `BLOCK_N`ï¼‰ï¼Œ`grid` å°±ä½¿ç”¨å½“å‰ä¼ å…¥çš„ tile å‚æ•°è®¡ç®—

kernel å¯åŠ¨æ—¶ï¼Œä¸å†æ‰‹åŠ¨ä¼ å…¥å‚ä¸ autotune çš„å‡ ä¸ªå‚æ•°ï¼ˆè¿™é‡Œæ˜¯ï¼š`BLOCK_M`, `BLOCK_N`, `num_warps` å’Œ `num_stages`ï¼‰ï¼Œè¿™äº›å‚æ•°å°†è‡ªåŠ¨ä¼ å…¥ã€‚

ä¿®æ”¹åçš„ kernel å¯åŠ¨å‡½æ•°å¦‚ä¸‹ï¼Œä»¥ `flash_attention_forward` ä¸ºä¾‹ï¼Œ`flash_attention_backward` åŒç†ã€‚

```python
def flash_attention_forward(Q, K, V, is_causal):
    """
    è´Ÿè´£åˆ†é… O/LSE å¹¶ launch forward kernel
    """
    B, H, S_q, D = Q.shape
    _, _, S_k, _ = K.shape

    device = Q.device
    dtype = Q.dtype
    O = torch.empty((B, H, S_q, D), dtype=dtype, device=device)
    LSE = torch.empty((B, H, S_q), dtype=torch.float32, device=device)

    grid = lambda META: (triton.cdiv(S_q, META['BLOCK_M']), B * H) # BLOCK_M ä» META å­—å…¸æå–
    flash_attention_forward_kernel[grid](
        Q, K, V, O, LSE,                     
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),  
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),  
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),  
        O.stride(0), O.stride(1), O.stride(2), O.stride(3), 
        LSE.stride(0), LSE.stride(1), LSE.stride(2),         
        1 / (D ** 0.5), 
        B, H, S_q, S_k, D,
        is_causal=is_causal, # ä¸æ‰‹åŠ¨ä¼ å…¥ BLOCK_M, BLOCK_N, num_warps å’Œ num_stages
    )
    return O, LSE
```

ç”±äºè¯¥å®éªŒæ˜¯åœ¨æ¶ˆè´¹çº§æ˜¾å¡ RTX 5060ti ä¸Šè¿›è¡Œçš„ï¼Œåœ¨ `fwd_bwd` æ¨¡å¼ä¸‹ï¼Œæœªä¼˜åŒ–çš„åŸå§‹å®ç°ä¸æ”¯æŒ `D=128` çš„æƒ…å†µï¼Œæ‰€ä»¥ä¸Šè¿°é…ç½®ä¸æ¶‰åŠ `D=128`ã€‚å½“ä½ åœ¨æ”¯æŒ `D=128` çš„ä¸“ä¸š GPU ï¼ˆH200ã€B200ç­‰ï¼‰ä¸Šè¿è¡Œæ—¶ï¼Œå»ºè®®è¿›è¡Œä»¥ä¸‹è°ƒæ•´ï¼š

* å°†è‡ªåŠ¨è°ƒä¼˜çš„æœç´¢èŒƒå›´è¿›ä¸€æ­¥æ‰©å¤§ï¼Œæ˜ç¡®åŒ…å« D = 128 è¿™ä¸€é€‰é¡¹
* è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„ BLOCK_M/N å€¼ï¼Œå› ä¸ºæ­¤æ—¶å†…å­˜å¸¦å®½å’Œå…±äº«å†…å­˜å®¹é‡çš„é™åˆ¶ä¼šç›¸åº”å‡è½»
* æ‰©å¤§ `num_warps` å’Œ `num_stages` è¿™ä¸¤ä¸ªå‚æ•°çš„èŒƒå›´ï¼Œè¿™åœ¨ä¸“ä¸šè®¾å¤‡ä¸Šé€šå¸¸æ˜¯å¯è¡Œå’Œæœ‰ç›Šçš„

æ€»ä¹‹ï¼Œåœ¨ä¸“ä¸šè®¾å¤‡ä¸Šçš„æœ€ä¼˜é…ç½®å¾€å¾€ä¸æˆ‘è¿™é‡Œç”¨æ¶ˆè´¹çº§ GPU å¾—åˆ°çš„æœ€ä¼˜é…ç½®ä¸åŒï¼Œå³ä¾¿ä½¿ç”¨çš„æ˜¯ç›¸åŒçš„ kernelã€‚æ‰€ä»¥å¤§å®¶è¦è®°å¾—æŒ‰ç…§è¿™äº›å»ºè®®ï¼Œåœ¨è‡ªå·±çš„è®¾å¤‡ä¸Šåšä¸€äº›ä¿®æ”¹å“¦ï¼

### 5.2.3 Auto-tune çš„ç»“æœ

<img src="./images/D_64_causal_fwd_bwd_autotune.png" style="zoom:33%;" />

* auto-tuning åçš„ç‰ˆæœ¬ç›¸æ¯”äºåŸå§‹å®ç°ï¼Œæ€§èƒ½æå‡åœ¨ 7% ~ 14% ä¹‹é—´
* è™½ç„¶æ€§èƒ½ç“¶é¢ˆæ²¡æœ‰å¾—åˆ°æ ¹æœ¬ç¼“è§£ï¼Œä½†è¿™ä¸ºæˆ‘ä»¬åç»­çš„ä¼˜åŒ–æ‰“å¥½äº†åŸºç¡€ï¼Œèƒ½å¤Ÿé¿å…å‚æ•°ä¸åŒ¹é…å¯¼è‡´çš„å¹²æ‰°



------

## 5.3 ä½¿ç”¨ TensorDescriptor

### 5.3.1 ä»€ä¹ˆæ˜¯ TensorDescriptor ï¼Ÿ

åœ¨è®²è§£ TensorDescriptor ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ä¼ ç»Ÿçš„ triton æ•°æ®åŠ è½½æ–¹å¼ï¼š

```python
# 1. å…ˆæ‰‹åŠ¨è®¡ç®—æ¯ä¸ªå…ƒç´ çš„ä½ç½®
q_ptrs = (
    Q_ptr
    + batch_idx * stride_qb 
    + head_idx * stride_qh 
    + Sq_offs[:, None] * stride_qs 
    + d_offs[None, :] * stride_qd
) 
# 2. åŠ è½½
Q_block = tl.load(q_ptrs, mask=mask_Sq[:, None], other=0.0)
```

è¿™ç§æ–¹å¼ï¼š

* æ¯ä¸ª program éœ€è¦æ˜¾å¼æ„é€ è®¿é—®åœ°å€ï¼Œé€šè¿‡ `tl.load` æŒ‡ä»¤ä» HBM åŠ è½½æ•°æ®
* è¿™äº›åœ°å€è®¡ç®—æœ¬èº«è™½ç„¶å¹¶ä¸ä¸€å®šæ˜¯ç“¶é¢ˆï¼Œä½†ä¼šå¼•å…¥é¢å¤–çš„æŒ‡ä»¤ä¸å¯„å­˜å™¨å ç”¨ï¼Œä¹Ÿè®©ç¼–è¯‘å™¨éš¾ä»¥ä»æ•´ä½“ä¸Šè¯†åˆ«â€œè¿™æ˜¯ä¸€æ¬¡è§„åˆ™çš„ tile è®¿é—®â€ï¼Œä»è€Œéš¾ä»¥åº”ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–ã€‚

å¦‚ä½•æ”¹å˜è¿™ä¸€åˆ‡å‘¢ï¼Ÿè¿™å°±è¦ç”¨åˆ° NVIDIA åœ¨ **Hopper æ¶æ„**ï¼ˆå¦‚ H100 / H200 GPUï¼‰ä¸­å¼•å…¥çš„ä¸€ä¸ªä¸“ç”¨ç¡¬ä»¶å•å…ƒï¼š**TMA (Tensor Memory Accelerator)** äº†ã€‚

TMA çš„ä¸»è¦ä½œç”¨æœ‰ä¸¤ç‚¹ï¼š

- åŠ é€Ÿ**æ•´å— tensor æ•°æ®**çš„ä¼ è¾“ â€”â€” ä» HBM åˆ° Shared Memory çš„**å¼‚æ­¥ã€é«˜æ•ˆ**ä¼ è¾“
- ç”±**ç¡¬ä»¶æ ¹æ®é¢„å…ˆæè¿°å¥½**çš„ tile å¸ƒå±€å®Œæˆåœ°å€ç”Ÿæˆä¸æ•°æ®æ¬è¿ï¼Œä»è€Œå‡å°‘åŠ¨æ€åœ°å€è®¡ç®—æŒ‡ä»¤å’Œæ§åˆ¶æµå¯¹è®¡ç®—ç®¡çº¿çš„å¹²æ‰°

è€Œæˆªè‡³ç›®å‰åœ¨ triton è¿™ä¸€å±‚ï¼Œæš´éœ²ç»™ç”¨æˆ·ã€èƒ½ç¨³å®šè¡¨è¾¾å¹¶è§¦å‘ TMA çš„æ¥å£å°±æ˜¯ `TensorDescriptor`ï¼Œæ™®é€šçš„ `tl.load` / `tl.make_block_ptr` ä»ç„¶å±äºä»¥â€œæŒ‡é’ˆ + è®¿é—®â€ä¸ºæ ¸å¿ƒçš„è®¿å­˜æ¨¡å‹ï¼Œè™½ç„¶ç¼–è¯‘å™¨å¯èƒ½è¿›è¡Œä¸€å®šä¼˜åŒ–ï¼Œä½†**å¹¶ä¸ç­‰ä»·äºâ€œä¿è¯èµ° TMAâ€**ã€‚

> TMA è¦æ±‚æ¬è¿çš„æ˜¯è§„åˆ™äºŒç»´/å¤šç»´ tileï¼Œå¹¶ä¸”å¸Œæœ›å½¢çŠ¶ã€æ­¥å¹…ã€è¾¹ç•Œå¤„ç†ï¼ˆpadding/è¶Šç•Œï¼‰åœ¨ç¼–è¯‘æœŸå°½å¯èƒ½æ˜ç¡®ï¼›
>
> `TensorDescriptor` æŠŠè¿™äº›ä¿¡æ¯å›ºå®šä¸‹æ¥ï¼Œé™ä½äº†åŠ¨æ€åœ°å€è®¡ç®—ä¸æ§åˆ¶æµï¼Œä»è€Œè®©åç«¯èƒ½åˆæ³•ã€å®‰å…¨åœ°ä½¿ç”¨ TMAã€‚

### 5.3.2 å¦‚ä½•ä½¿ç”¨ TensorDescriptor ï¼Ÿ

é¦–å…ˆåœ¨ python å±‚å®šä¹‰ï¼ŒæŒ‰ç…§é¡ºåºä¼ å…¥ï¼šçŸ©é˜µæœ¬èº«ï¼ŒçŸ©é˜µçš„å½¢çŠ¶ï¼Œæ¯ä¸ªç»´åº¦çš„ `stride`ï¼Œæ¬è¿çš„æ¯å—æ•°æ®çš„å½¢çŠ¶å’Œè¶Šç•Œè®¿é—®çš„å¡«å……å€¼ã€‚

ç„¶åï¼Œå°†å®šä¹‰å¥½çš„ desc_q ä¼ å…¥ kernelã€‚kernel å†…éƒ¨åŠ è½½æ—¶ä¼ å…¥æ•°æ®å—çš„èµ·ç‚¹å³å¯ã€‚

```python
# å‚è€ƒï¼šhttps://github.com/triton-lang/triton/blob/main/python/triton/tools/tensor_descriptor.py
from triton.tools.tensor_descriptor import TensorDescriptor

# 1. python å±‚ç›´æ¥å®šä¹‰
desc_q = TensorDescriptor(
    Q, shape=[B*H*S_q, D], strides=[D, 1], block_shape=[BLOCK_M, D], padding="zero"
)
# 2. ä¼ å…¥ kernel, kernel å†…åŠ è½½åªéœ€ä¼ å…¥æ•°æ®å—çš„èµ·ç‚¹å³å¯
@triton.jit
def flash_attention_forward_kernel(desc_q, ...):
    ...
    # load Q block
    qo_offsets_y = batch_idx * H * S_q + head_idx * S_q + pid_0 * BLOCK_M
    Q_block = desc_q.load([qo_offsets_y, 0])
    ...
```

è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªå°æŠ€å·§ï¼šå°† `[B, H, S_q, D]` è¿™ä¸ª 4D å¼ é‡å±•å¹³æˆ 2Dã€‚å¦‚æœä¸è¿™æ ·ï¼Œè€Œæ˜¯ä½¿ç”¨ 4D è¿›è¡Œå®šä¹‰ï¼Œé‚£å°±å˜æˆäº†ï¼š

```python
desc_q = TensorDescriptor(
    Q, shape=[B, H, S_q, D], strides=[H*S_q*D, S_q*D, D, 1], block_shape=[1, 1, BLOCK_M, D], padding="zero"
)
```

ä¸ä»…ç´¢å¼•è®¡ç®—å˜å¤æ‚ï¼Œè€Œä¸” block å½¢çŠ¶è¢«è¿«å˜æˆ `[1,1,BLOCK_M,D]`ï¼Œä½¿ç”¨æ—¶è¿˜è¦å…ˆ `squeeze` æ‰å‰ä¸¤ä¸ªç»´åº¦ã€‚

> `TensorDescriptor` æºç é‡Œæœ‰ç¡¬çº¦æŸï¼š
>
> - `rank = len(shape)`
> - `len(strides) == rank`
> - `len(block_shape) == rank`
> - `strides[-1] == 1` (Last dimension must be contiguous)

æ€»ä¹‹ï¼Œ4D æè¿°ä¼šå¼•å…¥ä¸¤ç»´â€œç©ºç»´åº¦â€ï¼Œè®© **block/ç´¢å¼•/å½¢çŠ¶å¤„ç†**éƒ½æ›´ç¹çï¼Œè€Œ **2D å±•å¹³**èƒ½ç›´æ¥æŠŠè¦ç”¨çš„å—è¡¨è¾¾æˆ `[BLOCK_M, D]`ã€‚å»ºè®®å¤§å®¶è®°ä½å¹¶ä½¿ç”¨è¿™ä¸ªå°æŠ€å·§ï¼

### 5.3.3 ä»£ç å®ç°

ä»¥ `flash_attention_forward` ä¸ºä¾‹ï¼Œ`flash_attention_backward` åŒç†ã€‚

```python
import triton
import triton.language as tl

# ==============================================================================
# Forward Pass
# ==============================================================================
def _host_descriptor_pre_hook_fwd(nargs):
    BLOCK_M = nargs["BLOCK_M"]
    BLOCK_N = nargs["BLOCK_N"]
    D = nargs["D"]

    nargs["desc_q"].block_shape = [BLOCK_M, D]
    nargs["desc_o"].block_shape = [BLOCK_M, D]
    nargs["desc_lse"].block_shape = [BLOCK_M]
    nargs["desc_k"].block_shape = [BLOCK_N, D]
    nargs["desc_v"].block_shape = [BLOCK_N, D]
    
@triton.autotune(
    configs = [
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=2, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=3, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=8, num_stages=2, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=8, num_stages=3, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=2, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}, num_warps=4, num_stages=3, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=2, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4, num_stages=3, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=2, pre_hook=_host_descriptor_pre_hook_fwd),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4, num_stages=3, pre_hook=_host_descriptor_pre_hook_fwd),
    ],
    key=['S_q', 'S_k', 'D', 'is_causal'],  # æ ¹æ®è¿™äº›å‚æ•°é€‰æ‹©æœ€ä¼˜é…ç½®
)

@triton.jit
def flash_attention_forward_kernel(
    # ----------------- TensorDescriptor ---------------
    desc_q, desc_k, desc_v, desc_o, desc_lse,

    # -------------------- ç¼©æ”¾å› å­ --------------------
    scale, # 1 / sqrt(D)

    # -------------------- ç»´åº¦ä¿¡æ¯ --------------------
    # Q: [B, H, S_q, D],  K/V: [B, H, S_k, D],  O: [B, H, S_q, D]
    B: tl.constexpr,       # batch size
    H: tl.constexpr,       # æ³¨æ„åŠ›å¤´æ•°
    S_q: tl.constexpr,     # åºåˆ—é•¿åº¦ (Q å’Œ O çš„è¡Œæ•°)
    S_k: tl.constexpr,     # åºåˆ—é•¿åº¦ (K/V çš„è¡Œæ•°)
    D: tl.constexpr,       # head_dim

    # -------------------- é…ç½®å‚æ•° ---------------------
    BLOCK_M: tl.constexpr, # Q_block çš„è¡Œæ•°
    BLOCK_N: tl.constexpr, # æµå¼æ‰«æ K/V çš„åˆ—å—å¤§å° (æ²¿ N ç»´)
    
    # -------------------- Flagå‚æ•° --------------------
    is_causal: tl.constexpr = False,
):
    """
    FlashAttention forward kernel
    """
    pid_0 = tl.program_id(0) 
    pid_1 = tl.program_id(1)
    batch_idx = pid_1 // H
    head_idx = pid_1 % H

    Sq_offs = pid_0 * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M,]

    # maskï¼šå¤„ç†æœ€åä¸€ä¸ª Q_block, å› ä¸ºå¯èƒ½è¶Šç•Œ, ç”¨äº score 
    # mask_Sq = Sq_offs < S_q
    
    # load Q block
    qo_offsets_y = batch_idx * H * S_q + head_idx * S_q + pid_0 * BLOCK_M
    Q_block = desc_q.load([qo_offsets_y, 0])

    # åˆå§‹åŒ–ç»Ÿè®¡é‡
    m = tl.full([BLOCK_M], float('-inf'), tl.float32)
    l = tl.zeros([BLOCK_M], tl.float32)
    o = tl.zeros([BLOCK_M, D], tl.float32)

    LOG2_E = 1.44269504 # log2(e), ç”¨äºtl.exp åˆ° tl.exp2 çš„è½¬åŒ–

    # é€å—å¤„ç† K/V block
    loop_end = (pid_0 + 1) * BLOCK_M if is_causal else S_k # causal æ¨¡å¼å¯ä»¥æå‰æˆªæ–­å¾ªç¯
    for start_s in range(0, loop_end, BLOCK_N):
        # load K/V block
        kv_offsets_y = batch_idx * H * S_k + head_idx * S_k + start_s
        K_block = desc_k.load([kv_offsets_y, 0]) # [BLOCK_N, D]
        V_block = desc_v.load([kv_offsets_y, 0]) # [BLOCK_N, D]

        Sk_offs = start_s + tl.arange(0, BLOCK_N)
        mask_Sk = Sk_offs < S_k
    
        # è®¡ç®— score, [BLOCK_M, D] @ [D, BLOCK_N] -> [BLOCK_M, BLOCK_N]
        S_block = tl.dot(Q_block, tl.trans(K_block)) * scale  # [BLOCK_M, BLOCK_N]
        S_block = tl.where(mask_Sk[None, :], S_block, float('-inf')) # [BLOCK_M, BLOCK_N]

        # å½“å‰ block å…¨éƒ¨èƒ½è¢«çœ‹è§çš„åˆ¤å®šæ¡ä»¶ï¼šQ blockçš„æœ€å°ç´¢å¼• >= K blockçš„æœ€å¤§ç´¢å¼•
        # éœ€è¦ causal mask çš„åˆ¤å®šæ¡ä»¶ï¼š        
        if is_causal:
            q_idx_min = pid_0 * BLOCK_M
            k_idx_max = start_s + BLOCK_N - 1
            if not (q_idx_min >= k_idx_max):
                causal_mask = Sq_offs[:, None] >= Sk_offs[None, :] # [BLOCK_M, BLOCK_N]
                S_block = tl.where(causal_mask, S_block, float('-inf')) # [BLOCK_M, BLOCK_N]

        # æ›´æ–°ç»Ÿè®¡é‡
        m_new = tl.maximum(m, tl.max(S_block, axis=1)) # [BLOCK_M,]

        correction = tl.exp2((m - m_new) * LOG2_E) # [BLOCK_M,]
        numerator = tl.exp2((S_block - m_new[:, None]) * LOG2_E) # [BLOCK_M, BLOCK_N]

        l = l * correction + tl.sum(numerator, axis=1) # [BLOCK_M,]
        # Tensor Core çš„çŸ©é˜µä¹˜æ³•è¦æ±‚è¾“å…¥ä¸ºåŠç²¾åº¦(æˆ– TF32)
        # å› æ­¤è¿™é‡Œå°† numerator è½¬å› fp16, ä½¿ tl.dot æœ‰æœºä¼šä½¿ç”¨ Tensor Core
        # è€Œç´¯åŠ ä»ç„¶åœ¨ fp32 çš„ o ä¸­å®Œæˆ, ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        o = o * correction[:, None] + tl.dot(numerator.to(tl.float16), V_block) # [BLOCK_M, D]

        m = m_new
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    o_final = o / l[:, None] 

    # write back to O_ptr
    desc_o.store([qo_offsets_y, 0], o_final) # [BLOCK_M, D]

    # è®¡ç®— LogSumExp
    lse = m + tl.log(l) # [BLOCK_M,]
    
    # write back to L_ptr
    desc_lse.store([qo_offsets_y], lse)
```

å¤§å®¶æœ‰æ²¡æœ‰å‘ç°ï¼Œä½¿ç”¨ `TensorDescriptor` æ—¶ï¼Œé™¤äº†æŠŠåŸæœ¬çš„â€œæŒ‡é’ˆ+è®¿å­˜â€çš„æ•°æ®åŠ è½½æ¨¡å¼ä¿®æ”¹ä¸ºä½¿ç”¨ `desc_q.load()` å’Œ `desc_o.store()` ä¹‹å¤–ï¼Œè¿˜å¢åŠ äº†ä¸€ä¸ª pre hook å‡½æ•°ï¼ˆ`_host_descriptor_pre_hook_fwd`ï¼‰ï¼Ÿ

è®²è§£è¿™ä¸ªå‡½æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ `flash_attention_forward_kernel` ä¿®æ”¹åçš„å¯åŠ¨å‡½æ•°ï¼š

```python
from triton.tools.tensor_descriptor import TensorDescriptor

def flash_attention_forward(Q, K, V, is_causal):
    """
    è´Ÿè´£åˆ†é… O/LSE å¹¶ launch forward kernel
    """
    B, H, S_q, D = Q.shape
    _, _, S_k, _ = K.shape

    device = Q.device
    dtype = Q.dtype
    O = torch.empty((B, H, S_q, D), dtype=dtype, device=device)#.contiguous()
    LSE = torch.empty((B, H, S_q), dtype=torch.float32, device=device)#.contiguous()

    # ç”±äºblock_shapeéœ€è¦ä½¿ç”¨ BLOCK_M ç­‰å‚æ•°ï¼Œè€Œè¿™äº›å‚æ•°åœ¨å¸¦ autotune çš„ kernel è°ƒç”¨åæ‰èƒ½è·å–
    # TensorDescriptor æ˜¯å…ˆåˆ›å»ºçš„, BLOCK_M æ˜¯åé€‰çš„,é‚£ descriptor çš„ block_shape å†™ä»€ä¹ˆ
    # å…ˆå†™ä¸€ä¸ª dummy block_shapeï¼Œç„¶åç”¨ pre_hook åœ¨ launch å‰è¡¥ä¸Šâ€œçœŸå®å€¼â€
    dummy_block_2D = [1, 1] 
    dummy_block_1D = [1] 

    # åˆ›å»º TensorDescriptor
    desc_q = TensorDescriptor(
        Q,
        shape=[B*H*S_q, D], # å±•å¹³æˆ 2D, åç»­ desc.load([y,0]) â†’ ç›´æ¥å¾—åˆ° [BLOCK_M,D], æ¯”ä½¿ç”¨ 4D æ›´ä¼˜
        strides=[D, 1],     # æ³¨æ„ï¼šstride[-1] å¿…é¡»æ˜¯ 1
        block_shape=dummy_block_2D, # å…ˆç”¨ dummy_block å ä½, æŠŠ TensorDescriptor å»ºèµ·æ¥
        padding="zero",
    )
    desc_k = TensorDescriptor(
        K, shape=[B*H*S_k, D], strides=[D, 1], block_shape=dummy_block_2D, padding="zero",
    )
    desc_v = TensorDescriptor(
        V, shape=[B*H*S_k, D], strides=[D, 1], block_shape=dummy_block_2D, padding="zero",
    )
    desc_o = TensorDescriptor(
        O, shape=[B*H*S_q, D], strides=[D, 1], block_shape=dummy_block_2D, padding="zero",
    )
    desc_lse = TensorDescriptor(
        LSE, shape=[B*H*S_q], strides=[1], block_shape=dummy_block_1D, padding="zero",
    )

    grid = lambda META: (triton.cdiv(S_q, META['BLOCK_M']), B * H)
    flash_attention_forward_kernel[grid](
        desc_q, desc_k, desc_v, desc_o, desc_lse,                              
        1 / (D ** 0.5), 
        B, H, S_q, S_k, D,
        is_causal=is_causal,
    )
    return O, LSE
```

å¯åŠ¨å‡½æ•°ä¹Ÿå˜å•¦ï¼æœ€å¤§çš„å˜åŒ–åœ¨äºï¼Œéœ€è¦åœ¨ host ç«¯å…ˆåˆ›å»ºå¹¶ä¼ å…¥ `TensorDescriptor`ã€‚ä½† `TensorDescriptor` çš„ `block_shape` é€šå¸¸ä¾èµ– autotune é€‰å‡ºçš„ tile å‚æ•°ï¼ˆå¦‚ `BLOCK_M/BLOCK_N`ï¼‰ã€‚å› æ­¤æˆ‘ä»¬ä¼šå…ˆç”¨ä¸€ä¸ªå ä½çš„ `block_shape` åˆ›å»º descriptorï¼Œç„¶ååˆ©ç”¨ `pre_hook` åœ¨ **æœ¬æ¬¡ launch ä¹‹å‰**ã€ä¸”å·²ç»ç¡®å®šäº†æœ¬æ¬¡ config çš„å‰æä¸‹ï¼ŒæŠŠ `block_shape` å†™æˆçœŸå®å€¼ã€‚

> æ³¨æ„ï¼š`pre_hook(nargs)` çš„ `nargs` é‡Œèƒ½æ‹¿åˆ°å“ªäº›å­—æ®µå–å†³äºåœ¨ host ç«¯è°ƒç”¨ kernel æ—¶ä¼ è¿›å»çš„ **æ‰€æœ‰ runtime å‚æ•°**ä»¥åŠæœ¬æ¬¡ autotune é€‰ä¸­çš„ **meta å‚æ•°**

### 5.3.4 å®éªŒç»“æœ

<img src="./images/D_64_causal_fwd_bwd_TensorDescriptor.png" style="zoom:33%;" />

* ç›¸æ¯”äºå•çº¯ä½¿ç”¨ autotuneï¼Œå¢åŠ  `TensorDescriptor` åï¼Œæ€§èƒ½æå‡ 10% å·¦å³



------

## 5.4 è®¡ç®—æµç¨‹ä¼˜åŒ–

æˆ‘ä»¬å›å¿†ä¸€ä¸‹åå‘ä¼ æ’­çš„ä¸¤ä¸ª kernelï¼š`flash_attention_dQ_kernel` å’Œ `flash_attention_dKV_kernel`

* äºŒè€…éƒ½éœ€è¦ä½¿ç”¨ `delta` çŸ©é˜µ

  ```python
  delta_block = tl.sum(dO_block.to(tl.float32) * O_block.to(tl.float32), axis=-1)
  ```

* ä¸”äºŒè€…éƒ½è®¡ç®—äº† `dalta`ï¼Œå°¤å…¶æ˜¯ `flash_attention_dKV_kernel` ä¸­ï¼Œ`delta` çš„è®¡ç®—ä½äºå¾ªç¯ä¸­ï¼Œéœ€è¦å¤šæ¬¡è®¡ç®—

è¿™ä¸ªè®¡ç®—æµç¨‹æœ‰æ²¡æœ‰å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–çš„å¯èƒ½å‘¢ï¼Ÿç»™å¤§å®¶ä¸€ä¸ªå°æç¤ºï¼š**ä¸¤ä¸ª kernel èƒ½ä¸èƒ½åªç®—ä¸€æ¬¡ `delta` ï¼Ÿ**

### 5.4.1 ä¼˜åŒ–æ–¹æ³•

åå‘ä¼ æ’­ kernel çš„æ‰§è¡Œé¡ºåºæ˜¯ `flash_attention_dQ_kernel â†’ flash_attention_dKV_kernel`ï¼Œå¯ä»¥ä¿å­˜ dQ_kernel è®¡ç®—çš„ `delta` çŸ©é˜µï¼Œåœ¨æ‰§è¡Œ dKV_kernel æ—¶ï¼Œç›´æ¥ä¼ å…¥ `delta`ã€‚

æ­¤æ–¹æ³•è¿˜å¸¦æ¥ä¸€ä¸ªéšè—ä¼˜åŒ–ï¼šç”±äº dKV_kernel ä¸å†éœ€è¦é‡æ–°è®¡ç®— `delta`ï¼Œæ‰€ä»¥å°±ä¸ç”¨åŠ è½½ `O` çŸ©é˜µäº†ï¼Œè¿›ä¸€æ­¥é™ä½äº†å¯„å­˜å™¨å‹åŠ›ã€‚

### 5.4.2 ä»£ç å®ç°

é¦–å…ˆï¼Œä¿®æ”¹ `flash_attention_dQ_kernel`ï¼Œåªéœ€å¢åŠ ä¿å­˜ `delta` çš„åŠŸèƒ½å³å¯ï¼š

```python
def _host_descriptor_pre_hook_bwd_dQ(nargs):
    BLOCK_M = nargs["BLOCK_M"]
    BLOCK_N = nargs["BLOCK_N"]
    D = nargs["D"]
	...
    nargs["desc_delta"].block_shape = [BLOCK_M] # pre hook å‡½æ•°å¢åŠ  desc_delta é¡¹
    
def flash_attention_dQ_kernel(
    # ----------------- TensorDescriptor ---------------
    ..., desc_delta, # è¾“å…¥å‚æ•°å¢åŠ ä¸€ä¸ª delta çš„ TensorDescriptor å®ä¾‹
	...
):
    ...
    # save delta
    desc_delta.store([qo_offsets_y], delta_block) # å¢åŠ  delta ä¿å­˜æ“ä½œ
```

å†ä¿®æ”¹ `flash_attention_dKV_kernel`ï¼š

```python
def _host_descriptor_pre_hook_bwd_dKV(nargs):
    BLOCK_M = nargs["BLOCK_M"]
    BLOCK_N = nargs["BLOCK_N"]
    D = nargs["D"]
    ...
    nargs["desc_delta"].block_shape = [BLOCK_M] # pre hook å‡½æ•°å¢åŠ  desc_delta é¡¹
    
def flash_attention_dKV_kernel(
    # ----------------- TensorDescriptor ---------------
    desc_q, desc_k, desc_v, desc_do, desc_lse, desc_dk, desc_dv, desc_delta, # å¢åŠ  desc_delta, åˆ é™¤ desc_o
	...
):
    ...
    # 6. ä» dQ kernel ä¿å­˜çš„ delta ç›´æ¥åŠ è½½
    delta_block = desc_delta.load([qo_offsets_y])  # ä»åŸæ¥çš„é‡æ–°è®¡ç®—ä¿®æ”¹ä¸ºç›´æ¥åŠ è½½, åŒæ—¶å»æ‰åŸæœ¬ O_blockçš„åŠ è½½
    ...
```

æœ€åä¿®æ”¹ kernel å¯åŠ¨å‡½æ•°ï¼š

```python
def flash_attention_backward(Q, K, V, O, dO, LSE, is_causal):
    ...
    delta = torch.empty((B, H, S_q), dtype=torch.float32, device=device) # åˆ›å»ºä¸€ä¸ªä¸­é—´å˜é‡

    # åˆ›å»º TensorDescriptor
    dummy_block_1D = [1] 
    ...
    desc_delta = TensorDescriptor(
        delta, shape=[B*H*S_q], strides=[1], block_shape=dummy_block_1D, padding="zero"
    )

    # è®¡ç®— dQ
    grid_Q = lambda META: (triton.cdiv(S_q, META['BLOCK_M']), B * H)
    flash_attention_dQ_kernel[grid_Q](
        desc_q, desc_k, desc_v, desc_do, desc_o, desc_lse, desc_dq, desc_delta, # å¢åŠ  desc_delta
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        is_causal=is_causal,
    )

    # è®¡ç®— dK/V
    grid_KV = lambda META: (triton.cdiv(S_k, META['BLOCK_N']), B * H)
    flash_attention_dKV_kernel[grid_KV](
        desc_q, desc_k, desc_v, desc_do, desc_lse, desc_dk, desc_dv, desc_delta, # å¢åŠ  desc_delta, åˆ é™¤ desc_o
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        is_causal=is_causal, 
    )

    return dQ, dK, dV
```

### 5.4.3 å®éªŒç»“æœ

<img src="./images/D_64_causal_fwd_bwd_delta.png" style="zoom:33%;" />

* åœ¨ä½¿ç”¨ TensorDescriptor çš„åŸºç¡€ä¸Šæ€§èƒ½å°å¹…åº¦æå‡

------

## 5.5 çœ‹ä¼¼å¯è¡Œçš„ä¼˜åŒ–æ‰‹æ®µ

æ¥ä¸‹æ¥å±•ç¤ºä¸¤ç§çœ‹èµ·æ¥å¯è¡Œï¼Œä½†æ˜¯å®é™…ä¸Šæ²¡ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚

### 5.5.1 å†…å­˜å¸ƒå±€â€œä¼˜åŒ–â€ â€”â€” é¢„è½¬ç½® K çŸ©é˜µ

å¤§å®¶åº”è¯¥æ³¨æ„åˆ°äº†ï¼Œæ— è®ºæ˜¯å‰å‘è¿˜æ˜¯åå‘ä¼ æ’­ï¼Œéƒ½ä¼šæ¶‰åŠåˆ°è®¡ç®— score çŸ©é˜µï¼š

```python
S_block = tl.dot(Q_block, tl.trans(K_block)) * scale
```

è¿™å¥ä»£ç ä¸­ï¼Œæ¶‰åŠåˆ°äº†å¯¹ **K çŸ©é˜µçš„è½¬ç½®**ï¼Œè€Œè½¬ç½®æ˜¯æœ‰å¼€é”€çš„ï¼Œå†åŠ ä¸Šè®¡ç®— score çŸ©é˜µé€šå¸¸å‡ºç°åœ¨å¾ªç¯å†…ï¼Œæ¯ä¸ª program éœ€è¦å¤„ç†å¾ˆå¤šæ¬¡è½¬ç½®ï¼Œè¿™å°±è¿›ä¸€æ­¥æ”¾å¤§äº†è½¬ç½®å¼€é”€ã€‚è¿™æ—¶ï¼Œå¤§å®¶å¯èƒ½ä¼šæƒ³åˆ°ï¼šèƒ½ä¸èƒ½åœ¨ä¼ å…¥ kernel å‰å°±å°† K çŸ©é˜µé¢„å…ˆè½¬ç½®å‘¢ï¼Ÿè¿™æ ·å²‚ä¸æ˜¯èŠ‚çº¦ä¸€å¤§ç¬”è½¬ç½®å¼€é”€ï¼èƒ½æƒ³åˆ°è¿™ä¸€ç‚¹ï¼Œå¤§å®¶å·²ç»å¾ˆæ£’å•¦ğŸ‘ğŸ‘

ä½†æ˜¯ï¼Œè¿™ä¸ªæƒ³æ³•çœŸçš„æ­£ç¡®å—ï¼Ÿè¿™å°±è¦æ¶‰åŠåˆ°**åœ¨ kernel å¤–è½¬ç½®**å’Œ**è¿è¡Œæ—¶åœ¨å¯„å­˜å™¨ä¸­è½¬ç½®**çš„å¼€é”€å¯¹æ¯”äº†ã€‚å¦‚æœæˆ‘ä»¬é€‰æ‹©åœ¨ kernel å¤–è¿›è¡Œè½¬ç½®ï¼Œå°±éœ€è¦åœ¨**æ˜¾å­˜**ä¸­è¿›è¡ŒçŸ©é˜µçš„è¯»å†™ï¼Œç›¸æ¯”äºåœ¨å¯„å­˜å™¨ä¸­è¿›è¡Œæ“ä½œï¼Œå‰è€…å¤ªæ…¢äº†ã€‚æ‰€ä»¥ï¼Œ**é¢„è½¬ç½® K çŸ©é˜µ**è¿™ä¸ªæ–¹æ³•ï¼Œå®é™…ä¸Šä¸å¯è¡Œï¼

å¤§å®¶å¯ä»¥è‡ªè¡ŒéªŒè¯ä¸€ä¸‹ï¼Œä½ ä¼šå‘ç°é¢„è½¬ç½®åæ€§èƒ½ä¸å‡åé™ã€‚

### 5.5.2 Warp Specialization

#### Warp Specialization æ˜¯ä»€ä¹ˆ ï¼Ÿ

>  `Warp Specialization` çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**è®©åŒä¸€ä¸ª thread block é‡Œçš„ä¸åŒ warpï¼Œé•¿æœŸå›ºå®šåœ°åšä¸åŒçš„äº‹æƒ…ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ warp è½®æµåšæ‰€æœ‰äº‹æƒ…ã€‚**

åœ¨ä¼ ç»Ÿ CUDA / Triton kernel é‡Œï¼Œä¸€ä¸ª block å†…çš„ warp å¾€å¾€æ˜¯**â€œåŒæ„çš„â€**ï¼ˆæˆ‘ä»¬è¿™é‡Œä¹Ÿæ˜¯ï¼‰ï¼š

- æ¯ä¸ª warp éƒ½æ‰§è¡ŒåŒæ ·çš„æŒ‡ä»¤æµ
- åªæ˜¯å¤„ç†ä¸åŒçš„æ•°æ® tile

`Warp specialization` åˆ™æ˜¯**â€œå¼‚æ„åˆ†å·¥â€**ã€‚åœ¨ FlashAttention è¿™æ ·çš„ç®—å­ä¸­ï¼Œ`Warp Specialization` é€šå¸¸ç”¨äºï¼š

- ä¸€éƒ¨åˆ† warp æŒç»­æ¬è¿ K/V æˆ–ç»Ÿè®¡é‡
- å¦ä¸€éƒ¨åˆ† warp ä¸“æ³¨äºçŸ©é˜µè®¡ç®—å’Œè¾“å‡ºç´¯åŠ 

> ä½†è¿™ç±»ä¼˜åŒ–é«˜åº¦ä¾èµ–åº•å±‚ç¡¬ä»¶è°ƒåº¦å’Œç¼–è¯‘å™¨æ”¯æŒï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªç†è§£å…¶æ€æƒ³ï¼Œè€Œä¸å¼ºæ±‚å®Œå…¨å¤ç°ã€‚

#### å¦‚ä½•åœ¨ triton ä¸­ä½¿ç”¨ ï¼Ÿ

æˆªè‡³ç›®å‰ï¼Œä¸»è¦é€šè¿‡åœ¨ `triton.language.range` ä¸­è®¾ç½® `warp_specialize` å‚æ•°æ¥æ§åˆ¶æ˜¯å¦ä½¿ç”¨ `Warp Specialization`ï¼Œä¾‹å¦‚ï¼š

```python
for start_s in tl.range(0, loop_end, BLOCK_N, warp_specialize=True):
    ...
```

å…·ä½“å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://triton-lang.org/main/python-api/generated/triton.language.range.html

ä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå®˜æ–¹æ–‡æ¡£æ˜ç¡®è¡¨ç¤ºï¼š

```markdown
Note that warp specialization is only supported on Blackwell GPUs and only works on simple matmul loops. Support for arbitrary loops will be expanded over time.
```

ä¹Ÿå°±æ˜¯è¯´æœ‰ä¸¤ç‚¹é™åˆ¶ï¼š

* å¿…é¡»æ˜¯ Blackwell æ¶æ„ GPUï¼ˆå¦‚ï¼šB200 GPU ç­‰ï¼‰
* ç›®å‰åªèƒ½ç”¨äºç®€å•çš„çŸ©é˜µè¿ç®—

#### FlashAttention å¯ä»¥ä½¿ç”¨å—ï¼Ÿ

è¿™æ˜¯å¤§å®¶æœ€å…³å¿ƒçš„é—®é¢˜ï¼Œä½†æ˜¯å¾ˆé—æ†¾ï¼Œè‡³å°‘ç›®å‰è¿˜ç”¨ä¸äº†ã€‚FlashAttention å°¤å…¶æ˜¯ backwardï¼Œæ¶‰åŠåˆ°çŸ©é˜µä¹˜ã€æµå¼ç´¯åŠ ã€æŒ‡æ•°è¿ç®—ç­‰ï¼Œå·²ç»è§¦åŠäº† Triton å½“å‰ `Warp Specialization` çš„ç¡¬é™åˆ¶ã€‚å¤§å®¶å¯ä»¥è‡ªè¡Œå°è¯•éªŒè¯ã€‚

ç»è¿‡æˆ‘çš„å®éªŒï¼Œå¦‚æœå¼ºè¡Œå¼€å¯ `Warp Specialization`ï¼Œåå‘ä¼ æ’­ä¼šé€ æˆ dQ å’Œ dK ä¸¤ä¸ªçŸ©é˜µè®¡ç®—é”™è¯¯ï¼Œè€Œå‰å‘ä¼ æ’­å³ä½¿è®¡ç®—æ­£ç¡®ï¼Œæ€§èƒ½ä¹Ÿæœ‰æ‰€ä¸‹é™ã€‚ä¸è¿‡æˆ‘ç›¸ä¿¡éšç€ triton è¿›ä¸€æ­¥å‘å±•ï¼Œè¿™ä¸ªåŠŸèƒ½æ€»ä¼šæœ‰èƒ½ç”¨åˆ° FlashAttention çš„ä¸€å¤©ï¼Œå¤§å®¶å¯ä»¥å…ˆè®°ä½ Warp Specialization è¿™ä¸ªä¼˜åŒ–æ‰‹æ®µã€‚

------

## 5.6 å…¨é¢æ€§èƒ½éªŒè¯

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬çš„ FlashAttention ç®—å­ä¼˜åŒ–å°±ç»“æŸäº†ï¼Œæ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å…¨é¢çš„éªŒè¯ä¸€ä¸‹åœ¨ä¸åŒå¤´ç»´åº¦ `D`ã€ä¸åŒæ¨¡å¼ã€ä¸åŒåºåˆ—é•¿åº¦ä»¥åŠæ˜¯å¦å¼€å¯ `is_causal` åœºæ™¯ä¸‹çš„ç®—å­æ€§èƒ½ï¼ŒåŒæ—¶å¯¹æ¯” **Naive attention** å’Œ **PyTorch SDPA**ã€‚

### 5.6.1 D = 64ï¼Œmode = 'fwd_bwd'ï¼Œis_causal = True / False

![](./images/D_64_fwd_bwd.png)

### 5.6.2 D = 64ï¼Œmode = 'fwd'ï¼Œis_causal = True / False

![](./images/D_64_fwd.png)

### 5.6.3 D = 64ï¼Œmode = 'bwd'ï¼Œis_causal = True / False

![](./images/D_64_bwd.png)

### 5.6.4 D = 128ï¼Œmode = 'fwd_bwd'ï¼Œis_causal = True / False

åœ¨æ€§èƒ½è°ƒä¼˜å‰ï¼Œç”±äºGPUå†…å­˜é™åˆ¶ï¼Œå‰å‘+åå‘ï¼ˆfwd+bwdï¼‰çš„æµ‹è¯•å‡åªèƒ½åœ¨å¤´ç»´åº¦ D=64 çš„æ¡ä»¶ä¸‹è¿›è¡Œï¼Œä½†æ˜¯ä¼˜åŒ–åï¼Œå·²ç»å¯ä»¥åœ¨ D=128 çš„æƒ…å†µä¸‹è¿è¡Œã€‚è¿™è¯´æ˜ä¼˜åŒ–ä¸ä»…ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜æå‡äº†ç®—å­çš„å¯æ‰©å±•æ€§ï¼

![](./images/D_128_fwd_bwd.png)

### 5.6.5 D = 128ï¼Œmode = 'fwd'ï¼Œis_causal = True / False

![](./images/D_128_fwd.png)

### 5.6.6 D = 128ï¼Œmode = 'bwd'ï¼Œis_causal = True / False

![](./images/D_128_bwd.png)

------

## 5.7 æ€§èƒ½è°ƒä¼˜å¤ç›˜

* **ä¸ºä»€ä¹ˆ Backward æ¯” Forward æ…¢ï¼Ÿ**

  Backward çš„æ€§èƒ½ä½äº Forward å¹¶éå®ç°é—®é¢˜ï¼Œè€Œæ˜¯ç®—æ³•ç»“æ„ä½¿ç„¶ã€‚ç›¸æ¯” Forward ä¸»è¦ç”±è§„åˆ™çš„çŸ©é˜µä¹˜å’Œ online softmax æ„æˆï¼ŒBackward éœ€è¦é‡å»º softmax æ¦‚ç‡ã€é¢å¤–çš„ reduceï¼ˆå¦‚ `delta`ï¼‰ï¼Œå¹¶åŒ…å«æ›´å¤šé˜¶æ®µæ€§çš„è®¡ç®—ä¸ä¾èµ–ã€‚è¿™äº›æ“ä½œéš¾ä»¥ç»„ç»‡æˆå•ä¸€é«˜æ•ˆçš„ GEMM ä¸»å¹²ï¼Œå› æ­¤å…¶å¯è¾¾åˆ°çš„ç®—æœ¯å¼ºåº¦å’Œå¹¶è¡Œåº¦å¤©ç„¶ä½äº Forwardï¼Œè¿™ä¸€ç°è±¡åœ¨å®˜æ–¹å®ç°ä¸­åŒæ ·å­˜åœ¨ã€‚

* **ä¸ºä»€ä¹ˆ Triton å¾ˆéš¾å®Œå…¨å¤ç°å®˜æ–¹ Backwardï¼Ÿ**

  å®˜æ–¹ FlashAttention Backward çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸»è¦æ¥è‡ª **persistent thread blocksã€warp specialization ç­‰æ¶æ„çº§è°ƒåº¦ä¼˜åŒ–**ï¼Œè€Œéç®—æ³•æœ¬èº«ã€‚è¿™äº›ä¼˜åŒ–ä¾èµ–å¯¹çº¿ç¨‹ç”Ÿå‘½å‘¨æœŸå’Œ SM å¸¸é©»çŠ¶æ€çš„ç²¾ç»†æ§åˆ¶ï¼Œè¶…å‡ºäº† Triton ä½œä¸º kernel-level DSL çš„è®¾è®¡è¾¹ç•Œã€‚å› æ­¤ï¼ŒTriton å®ç°å¯ä»¥é€¼è¿‘å…¶ç®—æ³•æ•ˆç‡ï¼Œä½†éš¾ä»¥åœ¨è°ƒåº¦å±‚é¢å®Œå…¨å¤ç°å®˜æ–¹å®ç°çš„æé™æ€§èƒ½ã€‚

* **ä¸ºä»€ä¹ˆè¯´å½“å‰æ€§èƒ½å·²ç»â€œè¶³å¤Ÿå¥½â€ï¼Ÿ**

  ä»æµ‹è¯•ç»“æœæ¥çœ‹ï¼Œæœ¬å®ç°çš„æ€§èƒ½æ›²çº¿éšåºåˆ—é•¿åº¦å¢é•¿ç¨³å®šæå‡å¹¶è¶‹äºé¥±å’Œï¼Œè¡¨æ˜ kernel å·²è¿›å…¥è®¡ç®—å—é™åŒºé—´ã€‚åœ¨ Forward åœºæ™¯ä¸‹ï¼Œæ€§èƒ½å·²æ¥è¿‘ç”šè‡³å±€éƒ¨è¿½å¹³ PyTorch å®˜æ–¹å®ç°ï¼›åœ¨ Backward åŠ fwd+bwd åœºæ™¯ä¸­ï¼Œç¨³å®šè¾¾åˆ°å…¶ 70%ï½80%ã€‚åœ¨ Triton å¯è¡¨è¾¾çš„ä¼˜åŒ–ç©ºé—´å†…ï¼Œè¿™ä¸€ç»“æœå·²æ¥è¿‘å¯å®ç°çš„ä¸Šé™ã€‚

* **å¦‚æœè¦æ›´è¿›ä¸€æ­¥ï¼ŒTriton åœºæ™¯ä¸‹æ˜¯å¦å¯è¡Œï¼Ÿ**

  åœ¨ Triton æ¡†æ¶å†…ï¼Œä»å¯é€šè¿‡æ›´æ¿€è¿›çš„ kernel fusionã€é…ç½®æœç´¢æˆ–å‡å°‘ä¸­é—´ç»“æœé‡å»ºè·å¾—æœ‰é™æå‡ï¼Œä½†ä¾èµ– persistent blocksã€warp çº§è§’è‰²åˆ†å·¥æˆ–è·¨ kernel å¸¸é©»çŠ¶æ€çš„ä¼˜åŒ–å¹¶ä¸å…·å¤‡å¯è¡Œæ€§ã€‚æ¢è¨€ä¹‹ï¼Œè¿›ä¸€æ­¥çš„å¤§å¹…æå‡éœ€è¦è¿›å…¥ CUDA/PTX å±‚çº§ï¼Œä¸å†æ˜¯ Triton çš„ç›®æ ‡ä½¿ç”¨åœºæ™¯ã€‚

> åœ¨ Triton çš„èƒ½åŠ›è¾¹ç•Œå†…ï¼Œè¯¥å®ç°å·²åœ¨å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ä¸æ€§èƒ½ä¹‹é—´å–å¾—äº†æ¥è¿‘æœ€ä¼˜çš„å¹³è¡¡ã€‚

------

## 5.8 æœ¬ç« å°ç»“

æœ¬ç« ä»â€œå…ˆæµ‹æ¸…æ¥šå†ä¼˜åŒ–â€çš„åŸåˆ™å‡ºå‘ï¼Œå…ˆå»ºç«‹äº†å¯å¤ç°çš„æ€§èƒ½åŸºçº¿ï¼šç”¨ GPU äº‹ä»¶è®¡æ—¶ã€å‰”é™¤å†·å¯åŠ¨å¼€é”€ï¼Œå¹¶ä»¥ TFLOPS ç»Ÿä¸€è¡¡é‡ä¸åŒå®ç°çš„ååã€‚åŸºçº¿å¯¹æ¯”æ¸…æ™°åœ°å±•ç¤ºäº†ï¼šnaive attention æ—¢æ…¢åˆä¸å¯æ‰©å±•ï¼Œè€ŒåŸºäº online softmax çš„æµå¼è®¡ç®—èƒ½é¿å…ä¸­é—´çŸ©é˜µç‰©åŒ–ï¼Œä½¿ç®—å­åœ¨é•¿åºåˆ—ä¸‹ä¿æŒç¨³å®šæ‰©å±•ï¼Œå¹¶é€æ­¥è¿›å…¥ compute-bound åŒºé—´ã€‚

åœ¨ä¼˜åŒ–è·¯å¾„ä¸Šï¼Œæˆ‘ä»¬ä¾æ¬¡å¼•å…¥äº†ä¸‰ç±»å…³é”®æ‰‹æ®µï¼š

* **Auto-tune** è§£è€¦æ€§èƒ½ä¸æ‰‹å·¥å‚æ•°é€‰æ‹©ï¼Œé¿å… BLOCK/warp/stage ä¸åŒ¹é…å¸¦æ¥çš„â€œä¼˜åŒ–å¤±çœŸâ€
* **TensorDescriptor + pre_hook** è®© tile è®¿å­˜è¡¨è¾¾æ›´è§„åˆ™ï¼Œä¸ºåç«¯è§¦å‘æ›´é«˜æ•ˆçš„æ•°æ®æ¬è¿ï¼ˆå¦‚ TMAï¼‰åˆ›é€ æ¡ä»¶
* **è®¡ç®—æµç¨‹é‡æ’**ï¼ˆå¤ç”¨å¹¶ç¼“å­˜ `delta`ï¼‰å‡å°‘å†—ä½™è®¡ç®—ä¸ä¸å¿…è¦çš„å¼ é‡åŠ è½½ï¼Œè¿›ä¸€æ­¥ç¼“è§£åå‘ä¼ æ’­çš„å¯„å­˜å™¨ä¸å¸¦å®½å‹åŠ›ã€‚

ä¸æ­¤åŒæ—¶ï¼Œæœ¬ç« ä¹Ÿåˆ»æ„å±•ç¤ºäº†ä¸¤ç±»â€œçœ‹ä¼¼åˆç†ä½†æ— æ•ˆâ€çš„æ€è·¯ï¼šé¢„è½¬ç½® K å—é™äºæ˜¾å­˜è¯»å†™æˆæœ¬ï¼Œè€Œ warp specialization ç›®å‰å—ç¡¬ä»¶ä¸ Triton æ”¯æŒèŒƒå›´é™åˆ¶ï¼Œå°šéš¾ç›´æ¥ç”¨äºå®Œæ•´ FlashAttentionï¼ˆå°¤å…¶ backwardï¼‰ã€‚

æœ€åçš„å…¨é¢è¯„æµ‹è¡¨æ˜ï¼š

* ä¼˜åŒ–åçš„å®ç°å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ä¸ç¨³å®šæ€§ â€”â€” Forward åœºæ™¯ä¸‹æ€§èƒ½å·²æ¥è¿‘ï¼ˆéƒ¨åˆ†é…ç½®ä¸‹å¯è¿½å¹³ï¼‰PyTorch SDPA
* Backward åŠ fwd+bwd åœºæ™¯ä¸­ç¨³å®šè¾¾åˆ°å…¶ 70%ï½80% åŒºé—´ã€‚

ç»¼åˆæ¥çœ‹ï¼Œåœ¨ Triton çš„è¡¨è¾¾è¾¹ç•Œå†…ï¼Œæˆ‘ä»¬å·²ç»åœ¨**æ€§èƒ½ã€å¯ç»´æŠ¤æ€§ä¸å¯è®²è§£æ€§**ä¹‹é—´å–å¾—äº†æ¥è¿‘æœ€ä¼˜çš„å¹³è¡¡ï¼







