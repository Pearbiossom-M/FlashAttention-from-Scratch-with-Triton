# Phase 2ï¼šFlashAttention çš„ç®—æ³•æ ¸å¿ƒ â€”â€” ç‰©ç†ä¸é€»è¾‘çš„åŒé‡çªå›´

ç›®æ ‡ï¼š

* å®Œæˆä¸€ä¸ª FlashAttention å‰å‘ä¼ æ’­çš„**æœ€å°å®ç°**ï¼ˆSingle Batchï¼ŒSingle Headï¼Œæ—  Causal Maskï¼‰

* è®©å¤§å®¶ç¬¬ä¸€æ¬¡çœŸæ­£å†™å‡ºæ¥ FlashAttention çš„çµé­‚ â€”â€” **Inner Loopï¼šonline softmax + æµå¼ç´¯åŠ  output**
* é€šè¿‡ç®—æ³•å˜é©ï¼ˆonline softmaxï¼‰ï¼ŒåŒæ—¶ç ´è§£ SRAM å®¹é‡çˆ†ç‚¸ï¼ˆ**ç‰©ç†ä¹‹å¢™**ï¼‰å’Œ Triton åŠ¨æ€åˆ‡ç‰‡é™åˆ¶ï¼ˆ**é€»è¾‘ä¹‹å¢™**ï¼‰

æ³¨æ„äº‹é¡¹ï¼š

* **Kernel å†…éƒ¨**ï¼šåªè®¡ç®—ä¸€ä¸ª `Q_block`ï¼ˆ`[BLOCK_M, K_dim]`ï¼‰ï¼Œç„¶åæ²¿ç€åºåˆ—ç»´åº¦ `N` æ‰«ææ‰€æœ‰ `K/V block`
* è™½ç„¶è¿™åªæ˜¯ FlashAttention å‰å‘ä¼ æ’­çš„æœ€å°å®ç°ï¼Œä½†å®ƒåŒ…å«äº† FlashAttention **æœ€å…³é”®ã€æœ€éš¾ç†è§£**çš„éƒ¨åˆ†ï¼š**â€œçœ‹åˆ°æ–° block æ—¶ï¼Œå¦‚ä½•ä¿®æ­£æ—§ softmax çš„å½’ä¸€åŒ–åŸºå‡†ï¼Œå¹¶åŒæ­¥ä¿®æ­£ç´¯åŠ çš„è¾“å‡ºå‘é‡ã€‚â€**

------

## 2.1 FlashAttention çš„æ ¸å¿ƒæ€è·¯ â€”â€” â€œå¦‚ä½•ç ´å¢™â€

åœ¨ Phase 1 ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°ä¸¤å µå¢™ï¼š

- **ç‰©ç†ä¹‹å¢™**ï¼š
   å®Œæ•´åŠ è½½ K/V æˆ–æŒæœ‰ `[BLOCK_M, N]` çš„ä¸­é—´çŸ©é˜µï¼ŒSRAM ä¸€å®šä¼šçˆ†
- **é€»è¾‘ä¹‹å¢™**ï¼š
   åœ¨ Triton ä¸­ï¼Œæ— æ³•å¯¹ä¸­é—´ tensorï¼ˆå¦‚ `p`ï¼‰åšåŠ¨æ€åˆ‡ç‰‡ï¼Œå¯¼è‡´å®Œæ•´æŒæœ‰ä¸­é—´çŸ©é˜µ `p` æ—¶ï¼Œå³ä½¿æƒ³é€šè¿‡å¯¹ V åˆ†å—æ¥é™ä½ SRAM å ç”¨ï¼Œç¼–è¯‘å™¨ä¹Ÿä¸å…è®¸ã€‚

FlashAttention çš„åšæ³•æ˜¯ä½¿ç”¨ **online softmax**ï¼š

* **ä¸å­˜å®Œæ•´çš„ä¸­é—´çŸ©é˜µï¼Œç°ç®—ç°ç”¨ï¼Œç”¨å®Œå³å¼ƒ**
* æ—¢ç„¶ä¸å­˜å®Œæ•´çš„ä¸­é—´çŸ©é˜µï¼Œè‡ªç„¶å°±ä¸å­˜åœ¨â€œåˆ‡ç‰‡â€è¿™ä¸ªæ“ä½œï¼Œ**é€»è¾‘ä¹‹å¢™**ä¹Ÿå°±è‡ªç„¶æ¶ˆå¤±äº†
* ç”±äº SRAM ä¸å­˜å®Œæ•´çŸ©é˜µï¼Œåªå­˜å½“å‰å¤„ç†çš„é‚£ä¸€å°å— K/V å’Œä¸­é—´çŸ©é˜µï¼Œå†…å­˜å ç”¨ä¹Ÿå°±ä¸å†éš N å¢é•¿ï¼Œ**ç‰©ç†ä¹‹å¢™**éšä¹‹æ¶ˆå¤±ã€‚

------

## 2.2 FlashAttention å‰å‘ä¼ æ’­çš„æœ€å°å®ç°

>è¿™é‡Œçš„â€œæœ€å°å®ç°â€æŒ‡çš„æ˜¯ï¼š**åœ¨ä¸å¼•å…¥ batch / head / causal mask ç­‰å·¥ç¨‹ç»´åº¦çš„å‰æä¸‹ï¼Œå®Œæ•´è¦†ç›– FlashAttention å‰å‘ä¼ æ’­çš„è®¡ç®—è¯­ä¹‰ã€‚**

è¯´æ˜ï¼š

* è¾“å…¥çŸ©é˜µ Qã€K å’Œ V çš„å½¢çŠ¶åˆ†åˆ«æ˜¯ [M, K_dim]ï¼Œ[N, K_dim] å’Œ [N, K_dim]
* è¿™é‡Œåªå– Q çš„ä¸€ä¸ª block å‚ä¸è®¡ç®—ï¼Œå½¢çŠ¶ä¸ºï¼š[BLOCK_M, K_dim]

**å…³é”®ï¼š**K/V ä»ç„¶å®Œæ•´å‚ä¸è®¡ç®—ï¼Œä½†**ä¸ä¸€æ¬¡æ€§åŠ è½½ï¼Œè€Œæ˜¯æ²¿ N åˆ†å—æ‰«æ**

### 2.2.1 Inner Loop ç®—æ³•æµç¨‹è¯¦è§£

#### æ­¥éª¤ä¸€ï¼šåˆå§‹åŒ–ç»Ÿè®¡é‡

online softmax çš„ç»Ÿè®¡é‡æ˜¯ **Per-row** çš„ï¼Œå› æ­¤å„ä¸ªç»Ÿè®¡é‡å¹¶ä¸æ˜¯æ ‡é‡ã€‚

```python
# å‡è®¾ Q_block çš„å½¢çŠ¶ä¸ºï¼š[BLOCK_M, K_dim]
m = -inf # shape: [BLOCK_M,]
l = 0    # shape: [BLOCK_M,]
o = 0    # shape: [BLOCK_M, K_dim]
```

* **m**ï¼šå½“å‰å·²å¤„ç†æ•°æ®çš„**æœ€å¤§å€¼**ï¼Œä½œä¸ºæ•°å€¼ç¨³å®šæ€§çš„é”šç‚¹ï¼Œæ‰€æœ‰ exp è®¡ç®—éƒ½ç›¸å¯¹äºè¿™ä¸ªåŸºå‡†
* **l**ï¼šå½’ä¸€åŒ–åˆ†æ¯ï¼Œå³ $\sum_j e^{s_{ij} - m_i} $ï¼Œç”¨äº**æœ€åçš„å½’ä¸€åŒ–**
* **o**ï¼š**æœªå½’ä¸€åŒ–**çš„è¾“å‡ºç´¯åŠ å™¨ï¼Œå³ $\sum_j e^{s_{ij} - m_i} \cdot V_j $ï¼Œç»´æŠ¤å½“å‰çš„"åŠ æƒå’Œ"ï¼Œæœ€åé™¤ä»¥ l å¾—åˆ°å½’ä¸€åŒ–çš„è¾“å‡º

#### æ­¥éª¤äºŒï¼šé€å—å¤„ç† K/V

```python
for start_n in range(0, N, BLOCK_N):
    # åŠ è½½å½“å‰ K block å’Œ V block
    K_block = K[start_n : start_n+BLOCK_N, :]  # [BLOCK_N, K_dim]
    V_block = V[start_n : start_n+BLOCK_N, :]  # [BLOCK_N, K_dim]
    
    # è®¡ç®—å±€éƒ¨ score
    scale = 1 / (q.size(-1) ** 0.5)
    s_block = Q_block @ K_block.T * scale  # [BLOCK_M, BLOCK_N]
    
    # æ›´æ–°ç»Ÿè®¡é‡ï¼ˆæ ¸å¿ƒï¼ï¼‰
    m_new = max(m, row_max(s_block))
    
    # ä¿®æ­£æ—§çš„ç´¯åŠ å™¨
    correction = exp(m - m_new)
    l = l * correction + row_sum(exp(s_block - m_new))
    o = o * correction + (exp(s_block - m_new) @ V_block)
    
    # æ›´æ–° m
    m = m_new
```

æ³¨æ„ï¼š

* `correction = exp(m - m_new)`ï¼Œä¸è¦æŠŠ `m` å’Œ `m_new` çš„ä½ç½®å†™åäº†å“¦ï¼

* å¦‚æœå¿˜è®°äº†è¿™ä¸ªå…¬å¼ï¼Œå¯ä»¥å‚è€ƒ Phase 0 çš„â€**0.2.3 Online Softmax çš„æ•°å­¦æ¨å¯¼**â€œ

* çœ‹ä¸æ˜ç™½ä¹Ÿæ²¡å…³ç³»ï¼Œå¯ä»¥è¿™æ ·ç†è§£ï¼š

  æƒ³ä¸€æƒ³ softmax çš„å…¬å¼ï¼ˆ `exp(score_block - m)` ï¼‰ï¼ŒåŸæœ¬çš„ç´¯åŠ å™¨ï¼ˆl å’Œ oï¼‰æ˜¯åŸºäºæ›´å°çš„ `m` è®¡ç®—çš„ï¼Œç›¸æ¯”äºçœŸå®å€¼ï¼Œæ˜¯ä¸æ˜¯å°±åå¤§äº†ï¼Ÿé‚£è¦ä¿®æ­£ï¼Œæ˜¯ä¸æ˜¯å°±éœ€è¦ä¹˜ä¸Šä¸€ä¸ªå°äº 1 çš„ä¿®æ­£å› å­ï¼Ÿé‚£ `exp(m - m_new)` å’Œ `exp(m_new - m)` å“ªä¸€ä¸ªå°äº 1 å‘¢ï¼Ÿå½“ç„¶æ˜¯ `exp(m - m_new)` äº†ï¼æ‰€ä»¥ï¼Œå°±æœ‰äº†ï¼š`correction = exp(m - m_new)`

#### æ­¥éª¤ä¸‰ï¼šæœ€ç»ˆå½’ä¸€åŒ–

```python
o_final = o / l[:, None] # o çš„æ¯ä¸€è¡Œéƒ½é™¤ä»¥å¯¹åº”çš„ l
```

æ³¨æ„ï¼š

* online softmax å¹¶ä¸ç”Ÿæˆä¸­é—´çŸ©é˜µ `p`ï¼Œå½’ä¸€åŒ–è¢«ç§»åŠ¨åˆ°äº†æœ€å

### 2.2.2 Python æ¨¡æ‹Ÿå®ç°

ç°åœ¨æˆ‘ä»¬æŠŠä¸Šè¿°æ­¥éª¤ä¸²èµ·æ¥ï¼Œä¾ç„¶å…ˆç”¨ python æ¨¡æ‹Ÿå®ç°ï¼Œä¾¿äºå¤§å®¶ç†è§£ã€‚

```python
import torch
import torch.nn.functional as F

def flash_attention_forward_sim(Q_block, K, V, BLOCK_N):
    """
    FlashAttention forward pass æ¨¡æ‹Ÿå®ç°
    Q_block: [BLOCK_M, K_dim]
    K, V: [N, K_dim]
    """
    BLOCK_M, K_dim = Q_block.shape
    N = K.size(0)
    scale = 1 / (K_dim ** 0.5)
    device = Q_block.device

    # åˆå§‹åŒ–ç»Ÿè®¡é‡
    m = torch.full((BLOCK_M,), float('-inf'), dtype=torch.float32, device=device)
    l = torch.zeros((BLOCK_M,), dtype=torch.float32, device=device)
    o = torch.zeros((BLOCK_M, K_dim), dtype=torch.float32, device=device)

    # é€å—å¤„ç† K/V
    for start_n in range(0, N, BLOCK_N):
        # åŠ è½½ K block å’Œ V block
        K_block = K[start_n: start_n+BLOCK_N, :] # [BLOCK_N, K_dim]
        V_block = V[start_n: start_n+BLOCK_N, :] # [BLOCK_N, K_dim]

        # è®¡ç®— score 
        s_block = Q_block.to(torch.float32) @ K_block.T.to(torch.float32) * scale # [BLOCK_M, BLOCK_N]

        # æ›´æ–°ç»Ÿè®¡é‡
        m_new = torch.maximum(m, s_block.max(dim=1)[0]) # [BLOCK_M,]

        correction = torch.exp(m - m_new) # [BLOCK_M,]
        numerator = torch.exp(s_block - m_new[:, None]) # [BLOCK_M, BLOCK_N]
        l = l * correction + torch.sum(numerator, dim=1) # [BLOCK_M,]
        o = o * correction[:, None] + numerator @ V_block.to(torch.float32) # [BLOCK_M, K_dim]
        
        m = m_new

    # æœ€ç»ˆå½’ä¸€åŒ–
    o_final = o / l[:, None] # [BLOCK_M, K_dim]
    return o_final
```

### 2.2.3 triton å®ç°

å°†ä¸Šè¿° python å®ç°ç¿»è¯‘ä¸º triton kernelï¼Œçœ‹çœ‹è¿˜ä¼šä¸ä¼šå‡ºç°é‚£å µ**â€é€»è¾‘ä¹‹å¢™â€œ**ï¼Ÿ

```python
import triton
import triton.language as tl

@triton.jit
def flash_attention_forward_kernel(
    # -------------------- æŒ‡é’ˆ --------------------
    Q_ptr, K_ptr, V_ptr, O_ptr, # è¾“å…¥è¾“å‡ºçŸ©é˜µæŒ‡é’ˆ

    # -------------------- stride --------------------
    stride_qm, stride_qk,  # Q åœ¨ä¸¤ä¸ªç»´åº¦ä¸Šçš„ stride
    stride_km, stride_kk,  # K åœ¨ä¸¤ä¸ªç»´åº¦ä¸Šçš„ stride
    stride_vm, stride_vk,  # V åœ¨ä¸¤ä¸ªç»´åº¦ä¸Šçš„ stride
    stride_om, stride_ok,  # O åœ¨ä¸¤ä¸ªç»´åº¦ä¸Šçš„ stride

    # -------------------- ç¼©æ”¾å› å­ --------------------
    scale, # 1 / sqrt(K_dim)
    
    # -------------------- ç»´åº¦ä¿¡æ¯ --------------------
    # Q:[M,K_dim], K:[N,K_dim], V:[N,K_dim], O:[M,K_dim]
    M,       # åºåˆ—é•¿åº¦ (Q çš„è¡Œæ•°)
    N: tl.constexpr,       # åºåˆ—é•¿åº¦ (K/V çš„è¡Œæ•°)
    K_dim: tl.constexpr,   # head_dim

    # -------------------- é…ç½®å‚æ•° ---------------------
    BLOCK_M: tl.constexpr, # Q_block çš„è¡Œæ•°
    BLOCK_N: tl.constexpr, # æµå¼æ‰«æ K/V çš„åˆ—å—å¤§å° (æ²¿ N ç»´)
):
    """
    FlashAttention forward pass (æœ€å°å®ç°)
    """
    pid_0 = tl.program_id(0)
    m_offs = pid_0 * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M,]
    k_offs = tl.arange(0, K_dim)                      # [K_dim,]

    # maskï¼šå¤„ç†æœ€åä¸€ä¸ª Q_block, å› ä¸ºå¯èƒ½è¶Šç•Œ
    mask_m = m_offs < M
    
    # load Q block
    q_ptrs = Q_ptr + m_offs[:, None] * stride_qm + k_offs[None, :] * stride_qk
    q_block = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)

    # åˆå§‹åŒ–ç»Ÿè®¡é‡
    m = tl.full([BLOCK_M], float('-inf'), tl.float32)
    l = tl.zeros([BLOCK_M], tl.float32)
    o = tl.zeros([BLOCK_M, K_dim], tl.float32)

    LOG2_E = 1.44269504 # log2(e), ç”¨äºtl.exp åˆ° tl.exp2 çš„è½¬åŒ–
    
    # é€å—å¤„ç† K/V block
    for start_n in range(0, N, BLOCK_N):
        # load K/V block
        n_offs = start_n + tl.arange(0, BLOCK_N)
        k_ptrs = K_ptr + n_offs[:, None] * stride_km + k_offs[None, :] * stride_kk
        v_ptrs = V_ptr + n_offs[:, None] * stride_vm + k_offs[None, :] * stride_vk
        mask_n = n_offs < N
        k_block = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0) # [BLOCK_N, K_dim]
        v_block = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0) # [BLOCK_N, K_dim]

        # è®¡ç®— score, [BLOCK_M, K_dim] @ [K_dim, BLOCK_N] -> [BLOCK_M, BLOCK_N]
        s = tl.dot(q_block, tl.trans(k_block)) * scale  # [BLOCK_M, BLOCK_N]
        s = tl.where(mask_n[None, :], s, float('-inf')) # [BLOCK_M, BLOCK_N]

        # æ›´æ–°ç»Ÿè®¡é‡
        m_new = tl.maximum(m, tl.max(s, axis=1)) # [BLOCK_M,]

        # ä½¿ç”¨ tl.exp2 æ¯” tl.exp æ›´å¿«
        # åœ¨å¾ˆå¤š GPU åç«¯é‡Œï¼Œexp2 å¾€å¾€æ¯” exp æ›´å®¹æ˜“æ˜ å°„åˆ°é«˜æ•ˆçš„å®ç°è·¯å¾„
		# å› æ­¤è¿™é‡Œç”¨ exp2(x * log2(e)) æ¥æ›¿ä»£ exp(x)
        # è¿™å±äºå¸¸è§çš„å·¥ç¨‹ä¼˜åŒ–ï¼Œç»†èŠ‚ä¾èµ–å…·ä½“æ¶æ„ä¸ç¼–è¯‘å™¨å®ç°
        correction = tl.exp2((m - m_new) * LOG2_E) # [BLOCK_M,]
        numerator = tl.exp2((s - m_new[:, None]) * LOG2_E) # [BLOCK_M, BLOCK_N]

        l = l * correction + tl.sum(numerator, axis=1) # [BLOCK_M,]
        o = o * correction[:, None] + tl.dot(numerator.to(tl.float16), v_block) # [BLOCK_M, K_dim]

        m = m_new
    
    # æœ€ç»ˆå½’ä¸€åŒ–
    o_final = o / l[:, None]

    # write back to O_ptr
    o_ptrs = O_ptr + m_offs[:, None] * stride_om + k_offs[None, :] * stride_ok
    tl.store(o_ptrs, o_final, mask=mask_m[:, None])
```

å¤§å®¶æ³¨æ„çœ‹ï¼Œä»£ç ä¸­å†ä¹Ÿæ²¡æœ‰å‡ºç°å¯¹ä¸­é—´çŸ©é˜µçš„åˆ‡ç‰‡æ“ä½œã€‚å› ä¸ºä¸­é—´çŸ©é˜µæœ¬èº«å°±æ˜¯åœ¨è¿™ä¸ª `for` å¾ªç¯é‡Œæ ¹æ®å½“å‰åˆ†å—è®¡ç®—å‡ºæ¥çš„**å±€éƒ¨å˜é‡**ã€‚æˆ‘ä»¬ä¸æ˜¯åœ¨åˆ‡åˆ†è›‹ç³•ï¼Œè€Œæ˜¯ä¸€æ¬¡åªçƒ¤å‡ºä¸€å°å—è›‹ç³•ç›´æ¥åƒæ‰ã€‚

**flash_attention_forward_kernel** çš„æˆåŠŸå®ç°ï¼Œè¯´æ˜ online softmax é€å—å¤„ç† K/Vï¼Œä¸å†ä¸€æ¬¡æ€§è®¡ç®— `p` çŸ©é˜µçš„æ€è·¯ï¼Œçš„ç¡®ä»æ ¹æœ¬ä¸Šé¿å…äº†åŠ¨æ€åˆ‡ç‰‡çš„éœ€æ±‚ï¼Œ**â€œé€»è¾‘ä¹‹å¢™â€**è‡ªåŠ¨ç“¦è§£ã€‚

è‡³äºå¦ä¸€å µå¢™ï¼Œå¤§å®¶è¿˜è®°å¾— **Phase 1 çš„ 1.2 èŠ‚**ä¸­ï¼Œæˆ‘ä»¬å¯¹**æ–¹æ¡ˆ A** åšçš„é‚£ä¸ªå®éªŒå—ï¼Ÿå½“æ—¶ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡å®éªŒæ¥éªŒè¯å¤§åºåˆ—é•¿åº¦æ˜¯å¦çœŸçš„ä¼šæ’‘çˆ† SRAMï¼Œå¹¶è®¾ç½®ï¼šK_dim=128ï¼ŒBLOCK_M=64ï¼Œä½¿ç”¨ RTX 5060ti è¿›è¡Œå®éªŒï¼Œå¾—åˆ°çš„ç»“æœæ˜¯ï¼Œå½“ N = 256 æ—¶ï¼Œç¨‹åºå°±å›  SRAM è¢«æ’‘çˆ†è€Œå´©æºƒã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬å¯ä»¥å†åšä¸€æ¬¡å®éªŒï¼š

è®¾ç½®ï¼šBLOCK_M=64ï¼ŒBLOCK_N=64ï¼ŒK_dim=128ã€‚ä¾æ—§ä½¿ç”¨ RTX 5060ti è¿›è¡Œå®éªŒï¼Œkernel çš„**è°ƒç”¨å‡½æ•°**ã€**æ­£ç¡®æ€§æ ¡éªŒåŸºå‡†**ä»¥åŠ**æ ¡éªŒå‡½æ•°**å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn.functional as F

# æ­£ç¡®æ€§æ ¡éªŒåŸºå‡†
def bench_attention(q, k, v):
    scale = 1 / (q.size(-1) ** 0.5)
    s = q @ k.transpose(-2, -1) * scale
    p = F.softmax(s, dim=-1)
    o = p @ v
    return o

# kernel çš„è°ƒç”¨å‡½æ•°
def launch_kernel(M, N, K_dim, BLOCK_M, BLOCK_N, device):
    
    Q = torch.randn((M, K_dim), dtype=torch.float16, device=device)
    K = torch.randn((N, K_dim), dtype=torch.float16, device=device)
    V = torch.randn((N, K_dim), dtype=torch.float16, device=device)
    O = torch.empty((M, K_dim), dtype=torch.float16, device=device)

    grid = (triton.cdiv(M, BLOCK_M),)
    flash_attention_forward_kernel[grid](
        Q, K, V, O,
        Q.stride(0), Q.stride(1),
        K.stride(0), K.stride(1),
        V.stride(0), V.stride(1),
        O.stride(0), O.stride(1),
        scale=1 / (K_dim ** 0.5),
        M=M, N=N, K_dim=K_dim,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,        
        num_warps=4,
        num_stages=3,
    )

    bench = bench_attention(
        Q.to(torch.float32), 
        K.to(torch.float32), 
        V.to(torch.float32)
    ).to(torch.float16)

    return O, bench

# æ­£ç¡®æ€§æ ¡éªŒå‡½æ•°
def verify_results(bench, triton_output, name="Attention"):
    # å°†ç»“æœè½¬ä¸º fp32 è¿›è¡ŒæŒ‡æ ‡è®¡ç®—ï¼Œé¿å…è®¡ç®—æŒ‡æ ‡æ—¶å¼•å…¥äºŒæ¬¡è¯¯å·®
    b = bench.to(torch.float32)
    t = triton_output.to(torch.float32)
    diff_abs = torch.abs(b - t)

    # 1. è®¡ç®—ç»å¯¹è¯¯å·®
    max_abs_err = torch.max(diff_abs).item()
    mean_abs_err = torch.mean(diff_abs).item()
    
    # 2. è®¡ç®—ç›¸å¯¹è¯¯å·® (åŠ ä¸Š epsilon é¿å…é™¤é›¶)
    rel_err = diff_abs / (torch.abs(b) + 1e-5)
    max_rel_err = torch.max(rel_err).item()
    
    # 3. ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_sim = torch.nn.functional.cosine_similarity(
        b.flatten(), t.flatten(), dim=0
    ).item()
    
    print(f"[{name} Verification]")
    print(f"Max Abs Error: {max_abs_err:.2e}")
    print(f"Mean Abs Error: {mean_abs_err:.2e}")
    print(f"Max Rel Error: {max_rel_err:.2e}")
    print(f"Cosine Similarity: {cosine_sim:.6f}")
    
    # 4. åˆ¤å®šæ ‡å‡†, å¯¹äº fp16: 
    is_allclose = torch.allclose(b, t, rtol=1e-2, atol=1e-3)
    
    if is_allclose and cosine_sim > 0.999:
        print("âœ… Test Passed!")
    else:
        print("âŒ Test Failed!")
```

æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œç»“æœå¦‚ä¸‹ï¼š

* å½“ N = 128ï¼Œæ­£ç¡®æ‰§è¡Œ
* å½“ N = 256ï¼Œæ­£ç¡®æ‰§è¡Œ
* å½“ N = 500ï¼Œæ­£ç¡®æ‰§è¡Œ
* â€¦â€¦
* å½“ N = 4096ï¼Œä»ç„¶æ­£ç¡®æ‰§è¡Œï¼Œä¸”ä¸ä¼š OOM

ç”±æ­¤å¯è§ï¼Œæˆ‘ä»¬çœŸçš„é€šè¿‡ **online softmax + æµå¼ç´¯åŠ  output** ç ´å¼€äº†ä¹‹å‰å›°æ‰°æˆ‘ä»¬çš„**â€ç‰©ç†ä¹‹å¢™â€œ** ã€‚

æ­å–œä½ ï¼Œå·²ç»å®Œæˆäº† FlashAttention æœ€è‰°éš¾çš„éƒ¨åˆ†ï¼ğŸ‘ğŸ‘

------

## 2.3 å°ç»“

æœ¬ç« æˆ‘ä»¬é€šè¿‡ **online softmax + æµå¼ç´¯åŠ  output** æˆåŠŸç ´å¼€äº†attention çš„ç‰©ç†ä¸é€»è¾‘ä¹‹å¢™ã€‚åŒæ—¶ä¹Ÿå¸¦å¤§å®¶å®Œæ•´å­¦ä¹ äº† FlashAttention å‰å‘ä¼ æ’­çš„**æœ€å°å®ç°**ï¼Œå¸Œæœ›å¤§å®¶å¯ä»¥ä»ä¸Šè¿° python å’Œ triton å®ç°ä¸­çœŸæ­£æŒæ¡ online softmax çš„ç®—æ³•æµç¨‹ã€‚

é‚£ä¹ˆç°åœ¨ï¼Œå¤§å®¶å¯ä»¥å°è¯•é—®è‡ªå·±ä¸¤ä¸ªé—®é¢˜ï¼Œï¼š

* FlashAttention ç›¸æ¯”äºæ ‡å‡† attention æ”¹å˜äº†å“ªä¸€æ­¥çš„è®¡ç®—é¡ºåºï¼Ÿ
* ä¸ºä»€ä¹ˆ Phase 1 çš„æ–¹æ¡ˆ Bï¼ˆQ/V åˆ†å—ä½†ä¸åˆ†å— Kï¼‰ä¼šåœ¨ Triton å·¥ç¨‹å±‚é¢å¡æ­»ï¼Œè€Œ Phase 2 å´èƒ½é¡ºç•…å®ç°ï¼Ÿ

å¦‚æœä½ å·²ç»å¯ä»¥è½»æ¾å›ç­”ï¼Œè¯·å…ˆç»™è‡ªå·±æŸä¸ªå¤§æ‹‡æŒ‡ğŸ‘ï¼Œå†è¯´ä¸€å¥ï¼šè€å·±ï¼Œä½ çœŸæ£’ï¼ç„¶åï¼Œå°±å¯ä»¥è¿›å…¥ä¸‹ä¸€ç« å•¦ã€‚åœ¨ä¸‹ä¸€ç« ï¼ˆPhase 3ï¼‰ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥è®©è¿™ä¸ª FlashAttention kernel èµ°å‘å·¥ä¸šçº§ï¼Œä¸ºå…¶å¢åŠ **é€šç”¨æ€§æ‰©å±•å’Œå·¥ç¨‹çº§ä¼˜åŒ–**ã€‚åŠ æ²¹ï¼
